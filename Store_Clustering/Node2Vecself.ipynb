{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import networkx as nx\n",
    "# import random\n",
    "# from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "# class Graph():\n",
    "#     def __init__(self, nx_G, is_directed, p, q):\n",
    "#         self.G = nx_G\n",
    "#         self.is_directed = is_directed\n",
    "#         self.p = p\n",
    "#         self.q = q\n",
    "\n",
    "#     def node2vec_walk(self, walk_length, start_node):\n",
    "#         '''\n",
    "#         Simulate a random walk starting from start node.\n",
    "#         '''\n",
    "#         G = self.G\n",
    "#         alias_nodes = self.alias_nodes\n",
    "#         alias_edges = self.alias_edges\n",
    "\n",
    "#         walk = [start_node]\n",
    "\n",
    "#         while len(walk) < walk_length:\n",
    "#             cur = walk[-1]\n",
    "#             cur_nbrs = sorted(G.neighbors(cur))\n",
    "#             if len(cur_nbrs) > 0:\n",
    "#                 if len(walk) == 1:\n",
    "#                     walk.append(cur_nbrs[alias_draw(alias_nodes[cur][0], alias_nodes[cur][1])])\n",
    "#                 else:\n",
    "#                     prev = walk[-2]\n",
    "#                     next = cur_nbrs[alias_draw(alias_edges[(prev, cur)][0], \n",
    "#                         alias_edges[(prev, cur)][1])]\n",
    "#                     walk.append(next)\n",
    "#             else:\n",
    "#                 break\n",
    "\n",
    "#         return walk\n",
    "\n",
    "#     def simulate_walks(self, num_walks, walk_length):\n",
    "#         '''\n",
    "#         Repeatedly simulate random walks from each node.\n",
    "#         '''\n",
    "#         G = self.G\n",
    "#         walks = []\n",
    "#         nodes = list(G.nodes())\n",
    "#         print ('Walk iteration:')\n",
    "#         for walk_iter in range(num_walks):\n",
    "#             print (str(walk_iter+1), '/', str(num_walks))\n",
    "#             random.shuffle(nodes)\n",
    "#             for node in nodes:\n",
    "#                 walks.append(self.node2vec_walk(walk_length=walk_length, start_node=node))\n",
    "\n",
    "#         return walks\n",
    "\n",
    "#     def get_alias_edge(self, src, dst):\n",
    "#         '''\n",
    "#         Get the alias edge setup lists for a given edge.\n",
    "#         '''\n",
    "#         G = self.G\n",
    "#         p = self.p\n",
    "#         q = self.q\n",
    "\n",
    "#         unnormalized_probs = []\n",
    "#         for dst_nbr in sorted(G.neighbors(dst)):\n",
    "#             if dst_nbr == src:\n",
    "#                 unnormalized_probs.append(G[dst][dst_nbr]['weight']/p)\n",
    "#             elif G.has_edge(dst_nbr, src):\n",
    "#                 unnormalized_probs.append(G[dst][dst_nbr]['weight'])\n",
    "#             else:\n",
    "#                 unnormalized_probs.append(G[dst][dst_nbr]['weight']/q)\n",
    "#         norm_const = sum(unnormalized_probs)\n",
    "#         normalized_probs =  [float(u_prob)/norm_const for u_prob in unnormalized_probs]\n",
    "\n",
    "#         return alias_setup(normalized_probs)\n",
    "\n",
    "#     def preprocess_transition_probs(self):\n",
    "#         '''\n",
    "#         Preprocessing of transition probabilities for guiding the random walks.\n",
    "#         '''\n",
    "#         G = self.G\n",
    "#         is_directed = self.is_directed\n",
    "\n",
    "#         alias_nodes = {}\n",
    "#         for node in G.nodes():\n",
    "#             unnormalized_probs = [G[node][nbr]['weight'] for nbr in sorted(G.neighbors(node))]\n",
    "#             norm_const = sum(unnormalized_probs)\n",
    "#             normalized_probs =  [float(u_prob)/norm_const for u_prob in unnormalized_probs]\n",
    "#             alias_nodes[node] = alias_setup(normalized_probs)\n",
    "\n",
    "#         alias_edges = {}\n",
    "#         triads = {}\n",
    "\n",
    "#         if is_directed:\n",
    "#             for edge in G.edges():\n",
    "#                 alias_edges[edge] = self.get_alias_edge(edge[0], edge[1])\n",
    "#         else:\n",
    "#             for edge in G.edges():\n",
    "#                 alias_edges[edge] = self.get_alias_edge(edge[0], edge[1])\n",
    "#                 alias_edges[(edge[1], edge[0])] = self.get_alias_edge(edge[1], edge[0])\n",
    "\n",
    "#         self.alias_nodes = alias_nodes\n",
    "#         self.alias_edges = alias_edges\n",
    "\n",
    "#         return\n",
    "\n",
    "\n",
    "# def alias_setup(probs):\n",
    "#     '''\n",
    "#     Compute utility lists for non-uniform sampling from discrete distributions.\n",
    "#     Refer to https://hips.seas.harvard.edu/blog/2013/03/03/the-alias-method-efficient-sampling-with-many-discrete-outcomes/\n",
    "#     for details\n",
    "#     '''\n",
    "#     K = len(probs)\n",
    "#     q = np.zeros(K)\n",
    "#     J = np.zeros(K, dtype=np.int)\n",
    "\n",
    "#     smaller = []\n",
    "#     larger = []\n",
    "#     for kk, prob in enumerate(probs):\n",
    "#         q[kk] = K*prob\n",
    "#         if q[kk] < 1.0:\n",
    "#             smaller.append(kk)\n",
    "#         else:\n",
    "#             larger.append(kk)\n",
    "\n",
    "#     while len(smaller) > 0 and len(larger) > 0:\n",
    "#         small = smaller.pop()\n",
    "#         large = larger.pop()\n",
    "\n",
    "#         J[small] = large\n",
    "#         q[large] = q[large] + q[small] - 1.0\n",
    "#         if q[large] < 1.0:\n",
    "#             smaller.append(large)\n",
    "#         else:\n",
    "#             larger.append(large)\n",
    "\n",
    "#     return J, q\n",
    "\n",
    "# def alias_draw(J, q):\n",
    "#     '''\n",
    "#     Draw sample from a non-uniform discrete distribution using alias sampling.\n",
    "#     '''\n",
    "#     K = len(J)\n",
    "\n",
    "#     kk = int(np.floor(np.random.rand()*K))\n",
    "#     if np.random.rand() < q[kk]:\n",
    "#         return kk\n",
    "#     else:\n",
    "#         return J[kk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import node2vec_lib\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_graph(weighted,directed,args):\n",
    "    '''\n",
    "    Reads the input network in networkx.\n",
    "    '''\n",
    "    if weighted:\n",
    "        G = nx.read_edgelist(args, nodetype=int, data=(('weight',float),), create_using=nx.DiGraph())\n",
    "    else:\n",
    "        G = nx.read_edgelist(args.input, nodetype=int, create_using=nx.DiGraph())\n",
    "        for edge in G.edges():\n",
    "            G[edge[0]][edge[1]]['weight'] = 1\n",
    "\n",
    "    if not directed:\n",
    "        G = G.to_undirected()\n",
    "\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.read_edgelist('/Users/s0p00zp/Documents/main/work/edgelist.txt', nodetype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for edge in G.edges():\n",
    "    G[edge[0]][edge[1]]['weight'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G= G.to_undirected()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import node2Vec1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(G.nodes())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[G[1][nbr]['weight'] for nbr in sorted(G.neighbors(1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = node2Vec1.Graph(G, False, 1., 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alias_nodes = {}\n",
    "for node in G.nodes():\n",
    "    unnormalized_probs = [G[node][nbr]['weight'] for nbr in sorted(G.neighbors(node))]\n",
    "    norm_const = sum(unnormalized_probs)\n",
    "    normalized_probs =  [float(u_prob)/norm_const for u_prob in unnormalized_probs]\n",
    "    alias_nodes[node] = alias_setup(normalized_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs=[0.25,0.25,0.25,0.25]\n",
    "K = len(probs)\n",
    "q = np.zeros(K)\n",
    "J = np.zeros(K, dtype=np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smaller = []\n",
    "larger = []\n",
    "for kk, prob in enumerate(probs):\n",
    "    q[kk] = K*prob\n",
    "    if q[kk] < 1.0:\n",
    "        smaller.append(kk)\n",
    "    else:\n",
    "        larger.append(kk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while len(smaller) > 0 and len(larger) > 0:\n",
    "    small = smaller.pop()\n",
    "    large = larger.pop()\n",
    "\n",
    "    J[small] = large\n",
    "    q[large] = q[large] + q[small] - 1.0\n",
    "    if q[large] < 1.0:\n",
    "        smaller.append(large)\n",
    "    else:\n",
    "        larger.append(large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alias_edges = {}\n",
    "triads = {}\n",
    "for edge in G.edges():\n",
    "    alias_edges[edge] = get_alias_edge(edge[0], edge[1])\n",
    "    alias_edges[(edge[1], edge[0])] = get_alias_edge(edge[1], edge[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alias_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.preprocess_transition_probs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "walks = G.simulate_walks(10, 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_embeddings(walks,dimensions,window_size,workers,iter):\n",
    "    '''\n",
    "    Learn embeddings by optimizing the Skipgram objective using SGD.\n",
    "    '''\n",
    "    walks = [list(map(str, walk)) for walk in walks]\n",
    "    model = Word2Vec(walks, size=dimensions, window=window_size, min_count=0, sg=1, workers=workers, iter=iter)\n",
    "    model.save_word2vec_format(args.output)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "walks = [list(map(str, walk)) for walk in walks]\n",
    "model = Word2Vec(walks, size=dimensions, window=window_size, min_count=0, sg=1, workers=workers, iter=iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions=128\n",
    "window_size=10\n",
    "workers=8\n",
    "iter=1\n",
    "from gensim.models import Word2Vec\n",
    "learn_embeddings(walks,dimensions,window_size,workers,iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(walks[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = {\n",
    "    'window_size': 2, # context window +- center word\n",
    "    'n': 10, # dimensions of word embeddings, also refer to size of hidden layer\n",
    "    'epochs': 50, # number of training epochs\n",
    "    'learning_rate': 0.01 # learning rate\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data(settings, corpus):\n",
    "    # Find unique word counts using dictonary\n",
    "    window=settings['window_size']\n",
    "    word_counts = dict()\n",
    "    for row in corpus:\n",
    "        for word in row:\n",
    "            word_counts[word] += 1\n",
    "    ## How many unique words in vocab? 9\n",
    "    v_count = len(word_counts.keys())\n",
    "    # Generate Lookup Dictionaries (vocab)\n",
    "    words_list = list(word_counts.keys())\n",
    "    # Generate word:index\n",
    "    word_index = dict((word, i) for i, word in enumerate(words_list))\n",
    "    # Generate index:word\n",
    "    index_word = dict((i, word) for i, word in enumerate(words_list))\n",
    "\n",
    "    training_data = []\n",
    "    # Cycle through each sentence in corpus\n",
    "    for sentence in corpus:\n",
    "        sent_len = len(sentence)\n",
    "        # Cycle through each word in sentence\n",
    "        for i, word in enumerate(sentence):\n",
    "            # Convert target word to one-hot\n",
    "            w_target = word2onehot(sentence[i])\n",
    "            # Cycle through context window\n",
    "            w_context = []\n",
    "            # Note: window_size 2 will have range of 5 values\n",
    "            for j in range(i - window, i + window+1):\n",
    "                # Criteria for context word \n",
    "                # 1. Target word cannot be context word (j != i)\n",
    "                # 2. Index must be greater or equal than 0 (j >= 0) - if not list index out of range\n",
    "                # 3. Index must be less or equal than length of sentence (j <= sent_len-1) - if not list index out of range \n",
    "                if j != i and j <= sent_len-1 and j >= 0:\n",
    "                    # Append the one-hot representation of word to w_context\n",
    "                    w_context.append(word2onehot(sentence[j]))\n",
    "                    # print(sentence[i], sentence[j]) \n",
    "                    # training_data contains a one-hot representation of the target word and context words\n",
    "            training_data.append([w_target, w_context])\n",
    "    return np.array(training_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alias_setup([0.25,0.25,0.25,0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2onehot(word):\n",
    "    \n",
    "    # word_vec - initialise a blank vector\n",
    "    word_vec = [0 for i in range(0, v_count)] # Alternative - np.zeros(self.v_count)\n",
    "    \n",
    "    # Get ID of word from word_index\n",
    "    word_index = word_index[word]\n",
    "    \n",
    "    # Change value from 0 to 1 according to ID of the word\n",
    "    word_vec[word_index] = 1\n",
    "    \n",
    "    return word_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(training_data):\n",
    "    # Initialising weight matrices\n",
    "    # getW1 - shape (9x10) and getW2 - shape (10x9)\n",
    "    #self.w1 = np.array(getW1)\n",
    "    #self.w2 = np.array(getW2)\n",
    "    w1 = np.random.uniform(-1, 1, (self.v_count, self.n)) # weight to be supplied by souradip\n",
    "    w2 = np.random.uniform(-1, 1, (self.n, self.v_count)) # weight to be supplied by souradip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import node2vec\n",
    "print(random.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(random.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
