{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/s0c02nj/anaconda2/envs/graph/lib/python3.6/site-packages/lightgbm/__init__.py:46: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.3) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import metrics, preprocessing\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from catboost import Pool, CatBoostClassifier\n",
    "import lightgbm as lgb\n",
    "import seaborn as sns\n",
    "#from itertools import izip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing\n",
    "train_data = pd.read_csv('/Users/s0c02nj/Desktop/OneDrive - Walmart Inc/WiDS/training_v2.csv')\n",
    "test_data =  pd.read_csv('/Users/s0c02nj/Desktop/OneDrive - Walmart Inc/WiDS/unlabeled.csv')\n",
    "\n",
    "sub_data =  pd.read_csv('/Users/s0c02nj/Desktop/OneDrive - Walmart Inc/WiDS/samplesubmission.csv')\n",
    "data_dict = pd.read_csv('/Users/s0c02nj/Desktop/OneDrive - Walmart Inc/WiDS/WiDS Datathon 2020 Dictionary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data.groupby('hospital_death').hist()\n",
    "#list_plot = list(np.arange(169,173))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = train_data.iloc[:,list_plot]\n",
    "# df['hospital_death'] = train_data['hospital_death']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.groupby('hospital_death').hist()\n",
    "#train_data[['height','hospital_death']].groupby('hospital_death').hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_classt(x): \n",
    "    if pd.isna(x):\n",
    "        return np.nan\n",
    "    elif x < 15: \n",
    "        return 'very severely underweight' \n",
    "    elif x >= 15 and x < 16: \n",
    "        return 'severely weight' \n",
    "    elif x >=16 and x < 18.5: \n",
    "        return 'underweight' \n",
    "    elif x >= 18.5 and x < 25: \n",
    "        return 'healthy weight' \n",
    "    elif x >= 25 and x < 30: \n",
    "        return 'overweight'\n",
    "    elif x >= 30 and x < 35: \n",
    "        return 'class 1' \n",
    "    elif x >= 35 and x < 40: \n",
    "        return 'class 2' \n",
    "    else: \n",
    "        return 'class 3' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['weightclass'] = train_data['bmi'].map(weighted_classt)\n",
    "test_data['weightclass'] = test_data['bmi'].map(weighted_classt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_var = list(data_dict[data_dict['Data Type'].isin(['binary','string','integer'])]['Variable Name'])\n",
    "cat_var.remove('icu_admit_type')\n",
    "cat_var.remove('bmi')\n",
    "cat_var = cat_var[1:]\n",
    "\n",
    "#to_label_encode = list(data_dict[data_dict['Data Type'].isin(['string'])]['Variable Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_var = cat_var + ['weightclass']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_var = list(data_dict[data_dict['Data Type'].isin(['numeric'])]['Variable Name'])\n",
    "cont_var.remove('pred')\n",
    "cont_var = cont_var + ['bmi']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_data.drop('hospital_death',axis=1)\n",
    "y_train = train_data['hospital_death']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = test_data.drop('hospital_death',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_comb = pd.concat([x_train,x_test],sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_drop = ['encounter_id', 'patient_id', 'readmission_status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Remove Features with more than 70 percent missing values\n",
    "#x_comb = x_comb.drop(['readmission_status'],axis = 1)\n",
    "x_comb = x_comb.drop(list_drop,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cat_var = list(set(cat_var).intersection(set(x_comb.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:03<00:00,  7.94it/s]\n"
     ]
    }
   ],
   "source": [
    "for col in tqdm(new_cat_var):\n",
    "    le = LabelEncoder()\n",
    "    x_comb[col] = le.fit_transform(x_comb[col].astype(str))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_comb[\"apache_4a_hospital_death_prob\"] = x_comb[\"apache_4a_hospital_death_prob\"].replace(-1, np.nan)\n",
    "x_comb[\"apache_4a_icu_death_prob\"] =  x_comb[\"apache_4a_icu_death_prob\"].replace(-1, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 131021/131021 [00:01<00:00, 71848.75it/s]\n"
     ]
    }
   ],
   "source": [
    "age_cat = []\n",
    "\n",
    "for i in tqdm(range(0,len(x_comb))):\n",
    "    \n",
    "    val = x_comb['age'].iloc[i]\n",
    "    \n",
    "    if val >= 15 and val <= 24: \n",
    "        age_cat.append('igen')\n",
    "    \n",
    "    elif val >= 25 and val <= 54: \n",
    "       age_cat.append('Prime_working_Age')\n",
    "    \n",
    "    elif val >= 55 and val <= 64: \n",
    "        age_cat.append('Mature_working_Age')\n",
    "        \n",
    "    else: \n",
    "        age_cat.append('Elderly_working_Age')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_comb['age_category'] = age_cat\n",
    "le = LabelEncoder()\n",
    "x_comb['age_category'] = le.fit_transform(x_comb['age_category'].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_cols_nans = list(data_dict[data_dict['Category'].isin(['vitals','labs','labs blood gas'])]['Variable Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_cols_nans = list(set(list_cols_nans).intersection(set(x_comb.columns)))\n",
    "#list_cols_nans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_comb1 = x_comb.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_comb1['unique_id'] = range(0,len(x_comb1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_comb2 = x_comb1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_comb2_tr = x_comb2[x_comb2['apache_4a_hospital_death_prob'].notnull()]\n",
    "x_comb2_pred = x_comb2[x_comb2['apache_4a_hospital_death_prob'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_demo_tr = x_comb2_tr.drop('apache_4a_hospital_death_prob',axis=1)\n",
    "x_demo_te = x_comb2_pred.drop('apache_4a_hospital_death_prob',axis=1)\n",
    "\n",
    "y_demo_tr = x_comb2_tr['apache_4a_hospital_death_prob']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_params = {'boost': 'gbdt',\n",
    "            'learning_rate': 0.01,\n",
    "            'max_depth':6,\n",
    "            'metric':{'rmsle'},\n",
    "            'num_threads': -1,\n",
    "            'objective': 'regression',\n",
    "            'verbosity': 1,\n",
    "            'num_leaves':15,\n",
    "            'bagging_fraction':0.5, #rows\n",
    "            'bagging_frequency':5,\n",
    "            'min_data_in_leaf':50,\n",
    "            'feature_fraction':0.3, # features\n",
    "            'lambda_l1':10,\n",
    "            'lambda_l2':15,\n",
    "            'bagging_seed':2019,\n",
    "            'min_gain_split':0.3\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(x_demo_tr, y_demo_tr, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train = lgb.Dataset(x_train, label=y_train)\n",
    "d_valid = lgb.Dataset(x_valid, label=y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_clf_demo = lgb.train(reg_params,\n",
    "                    d_train,\n",
    "                    10000,\n",
    "                    valid_sets = [d_train, d_valid],\n",
    "                    verbose_eval=100\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "apache_4a_pred = lgb_clf_demo.predict(x_demo_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_demo_te.index = range(0,len(x_demo_te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16550/16550 [00:00<00:00, 68276.67it/s]\n"
     ]
    }
   ],
   "source": [
    "dict_map = {}\n",
    "\n",
    "for i in tqdm(range(0,len(x_demo_te))):\n",
    "    u_id = x_demo_te['unique_id'].iloc[i]\n",
    "    dict_map[u_id] = apache_4a_pred[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_comb_imp = x_comb1[['apache_4a_hospital_death_prob','unique_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_keys = list(dict_map.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/131021 [00:00<?, ?it/s]/Users/s0c02nj/anaconda2/envs/graph/lib/python3.6/site-packages/pandas/core/indexing.py:205: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n",
      "/Users/s0c02nj/anaconda2/envs/graph/lib/python3.6/site-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "100%|██████████| 131021/131021 [00:46<00:00, 2836.90it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(0,len(x_comb_imp))):\n",
    "    \n",
    "    u_id = x_comb_imp['unique_id'].iloc[i]\n",
    "    \n",
    "    if u_id in list_keys :\n",
    "        \n",
    "        ap_val = dict_map[u_id]\n",
    "        \n",
    "        if ap_val >0 and ap_val <1:\n",
    "            \n",
    "            x_comb_imp['apache_4a_hospital_death_prob'].iloc[i] = ap_val\n",
    "        \n",
    "#         elif ap_val<0:\n",
    "#             x_comb_imp['apache_4a_hospital_death_prob'].iloc[i] = 0\n",
    "        \n",
    "#         elif ap_val>0:\n",
    "#             x_comb_imp['apache_4a_hospital_death_prob'].iloc[i] = 1\n",
    "        \n",
    "        \n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_comb['apache_4a_hospital_death_prob'] = x_comb_imp['apache_4a_hospital_death_prob']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30it [00:01, 16.92it/s]\n"
     ]
    }
   ],
   "source": [
    "count_var = []\n",
    "for i,col in tqdm(enumerate(new_cat_var)):\n",
    "    counter = Counter(x_comb[col])\n",
    "    x_comb[str(col)+'count'] = x_comb[col].apply(lambda x:counter[x])\n",
    "    count_var.append(str(col)+'count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List to be biined -----> VITALS\n",
    "#to_binned_columns = list(data_dict[data_dict['Category'] == 'labs blood gas']['Variable Name'].unique())\n",
    "to_binned_columns = list(data_dict[data_dict['Category'] == 'labs']['Variable Name'].unique())\n",
    "to_binned_columns = list(set(to_binned_columns).intersection(set(x_comb.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:01<00:00, 40.28it/s]\n"
     ]
    }
   ],
   "source": [
    "binned_cols = []\n",
    "\n",
    "for cols in tqdm(to_binned_columns) :\n",
    "    x_comb[cols+'_binned'] = list(pd.cut(x_comb[cols], 10 ,labels=False))\n",
    "    binned_cols.append(cols+'_binned')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_comb['height'+'_binned'] = list(pd.cut(x_comb['height'], 8,labels=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "featured_cols = binned_cols + ['age_category','height'+'_binned']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2way interaction\n",
    "bias_cols = ['gender','age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:14<00:00,  7.46s/it]\n"
     ]
    }
   ],
   "source": [
    "inter_cols6 = []\n",
    "\n",
    "for i in tqdm(bias_cols):\n",
    "    \n",
    "    for j in binned_cols :\n",
    "        \n",
    "        x_comb[i+'_'+j] = x_comb[i].astype(str) + '_' + x_comb[j].astype(str)\n",
    "        \n",
    "        inter_cols6.append(i+'_'+j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Critical -3way interaction\n",
    "\n",
    "\n",
    "x_comb['age'+'gender'+'weighted_classt'] = x_comb['age'].astype(str) + '_' + x_comb['gender'].astype(str) + '_' + x_comb['weightclass'].astype(str)\n",
    "x_comb['age'+'gender'+'ethnicity'] = x_comb['age'].astype(str) + '_' + x_comb['gender'].astype(str) + '_' + x_comb['ethnicity'].astype(str)\n",
    "x_comb['age'+'gender'+'ethnicity' +'weighted_classt' ] = x_comb['age'].astype(str) + '_' + x_comb['gender'].astype(str) + '_' + x_comb['ethnicity'].astype(str) +'_' + x_comb['weightclass'].astype(str)\n",
    "\n",
    "\n",
    "inter_cols6 = inter_cols6 + ['age'+'gender'+'weighted_classt'] +['age'+'gender'+'ethnicity'] + ['age'+'gender'+'ethnicity'+'weighted_classt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 123/123 [00:16<00:00,  7.32it/s]\n"
     ]
    }
   ],
   "source": [
    "for cols in tqdm(inter_cols6):\n",
    "    le = LabelEncoder()\n",
    "    x_comb[cols] = le.fit_transform(x_comb[cols].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_illness = ['aids',\n",
    "                'cirrhosis',\n",
    "                'diabetes_mellitus',\n",
    "                'hepatic_failure',\n",
    "                'immunosuppression',\n",
    "                'leukemia',\n",
    "                'lymphoma',\n",
    "                'solid_tumor_with_metastasis']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:05<00:00,  1.58it/s]\n"
     ]
    }
   ],
   "source": [
    "inter_cols1 = []\n",
    "\n",
    "for col in tqdm(list_illness) :\n",
    "    x_comb['hospital_id'+str(col)] = x_comb['hospital_id'].astype(str) + '_' + x_comb[col].astype(str)\n",
    "    x_comb['gender'+str(col)] = x_comb['gender'].astype(str) + '_' + x_comb[col].astype(str)\n",
    "    x_comb['age'+str(col)] = x_comb['age'].astype(str) + '_' + x_comb[col].astype(str)\n",
    "    x_comb['bmi'+str(col)] = x_comb['bmi'].astype(str) + '_' + x_comb[col].astype(str)\n",
    "    \n",
    "    inter_cols1.append('hospital_id'+str(col))\n",
    "    inter_cols1.append('gender'+str(col))\n",
    "    inter_cols1.append('age'+str(col))\n",
    "    inter_cols1.append('bmi'+str(col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:01<00:00, 23.97it/s]\n"
     ]
    }
   ],
   "source": [
    "for cols in tqdm(inter_cols1):\n",
    "    le = LabelEncoder()\n",
    "    x_comb[cols] = le.fit_transform(x_comb[cols])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:01<00:00,  6.45it/s]\n"
     ]
    }
   ],
   "source": [
    "inter_cols2 = []\n",
    "\n",
    "for col in tqdm(list_illness) :\n",
    "    x_comb['age_category'+str(col)] = x_comb['age_category'].astype(str) + '_' + x_comb[col].astype(str)\n",
    "    inter_cols2.append('age_category'+str(col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:00<00:00, 34.64it/s]\n"
     ]
    }
   ],
   "source": [
    "for cols in tqdm(inter_cols2):\n",
    "    le = LabelEncoder()\n",
    "    x_comb[cols] = le.fit_transform(x_comb[cols])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_train = x_train.copy()\n",
    "temp_train['hospital_death'] = train_data['hospital_death']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "4it [00:00, 33.82it/s]\u001b[A\u001b[A\n",
      "\n",
      "8it [00:00, 35.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "13it [00:00, 37.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "18it [00:00, 39.16it/s]\u001b[A\u001b[A\n",
      "\n",
      "22it [00:00, 39.11it/s]\u001b[A\u001b[A\n",
      "\n",
      "30it [00:00, 42.35it/s]\u001b[A\u001b[A\n"
     ]
    }
   ],
   "source": [
    "cat_count_var = []\n",
    "\n",
    "for i,col in tqdm(enumerate(new_cat_var)):\n",
    "    mean = temp_train['hospital_death'].mean()\n",
    "    \n",
    "    #Compute the number of values and the mean of each group\n",
    "    agg = temp_train.groupby(col)['hospital_death'].agg(['count', 'mean'])\n",
    "    counts = agg['count']\n",
    "    means =  agg['mean']\n",
    "    \n",
    "    #Compute the \"smoothed\" means\n",
    "    m= 3\n",
    "    smooth = (counts * means + m * mean) / (counts + m)\n",
    "    \n",
    "    #Final_val\n",
    "    x_comb[str(col)+'count_new'] = x_comb[col].map(smooth)\n",
    "    cat_count_var.append(str(col)+'count_new')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggs = {}\n",
    "# aggs['apache_4a_hospital_death_prob'] = ['median', 'mean', 'max', 'min']\n",
    "# aggs['apache_4a_icu_death_prob'] = ['median', 'mean', 'max', 'min']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cat_var = list(set(new_cat_var).intersection(x_comb.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "0it [00:00, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "3it [00:00,  9.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "4it [00:00,  6.43it/s]\u001b[A\u001b[A\n",
      "\n",
      "5it [00:00,  5.17it/s]\u001b[A\u001b[A\n",
      "\n",
      "6it [00:01,  4.50it/s]\u001b[A\u001b[A\n",
      "\n",
      "7it [00:01,  4.13it/s]\u001b[A\u001b[A\n",
      "\n",
      "8it [00:01,  3.97it/s]\u001b[A\u001b[A\n",
      "\n",
      "9it [00:02,  3.83it/s]\u001b[A\u001b[A\n",
      "\n",
      "10it [00:02,  3.77it/s]\u001b[A\u001b[A\n",
      "\n",
      "11it [00:02,  3.76it/s]\u001b[A\u001b[A\n",
      "\n",
      "12it [00:02,  3.64it/s]\u001b[A\u001b[A\n",
      "\n",
      "13it [00:03,  3.49it/s]\u001b[A\u001b[A\n",
      "\n",
      "14it [00:03,  3.48it/s]\u001b[A\u001b[A\n",
      "\n",
      "15it [00:03,  3.41it/s]\u001b[A\u001b[A\n",
      "\n",
      "16it [00:04,  3.38it/s]\u001b[A\u001b[A\n",
      "\n",
      "17it [00:04,  3.44it/s]\u001b[A\u001b[A\n",
      "\n",
      "18it [00:04,  3.46it/s]\u001b[A\u001b[A\n",
      "\n",
      "19it [00:04,  3.51it/s]\u001b[A\u001b[A\n",
      "\n",
      "20it [00:05,  3.52it/s]\u001b[A\u001b[A\n",
      "\n",
      "21it [00:05,  3.60it/s]\u001b[A\u001b[A\n",
      "\n",
      "22it [00:05,  3.42it/s]\u001b[A\u001b[A\n",
      "\n",
      "23it [00:06,  3.39it/s]\u001b[A\u001b[A\n",
      "\n",
      "24it [00:06,  3.33it/s]\u001b[A\u001b[A\n",
      "\n",
      "25it [00:06,  3.36it/s]\u001b[A\u001b[A\n",
      "\n",
      "26it [00:07,  3.22it/s]\u001b[A\u001b[A\n",
      "\n",
      "27it [00:07,  3.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "28it [00:07,  3.23it/s]\u001b[A\u001b[A\n",
      "\n",
      "29it [00:07,  3.22it/s]\u001b[A\u001b[A\n",
      "\n",
      "30it [00:08,  3.63it/s]\u001b[A\u001b[A\n"
     ]
    }
   ],
   "source": [
    "# for col in tqdm(new_cat_var):\n",
    "    \n",
    "#     aggs_temp = aggs.copy()\n",
    "#     #aggs_temp.pop(col)\n",
    "    \n",
    "#     agg_df = x_comb.groupby(col).agg(aggs_temp).reset_index()\n",
    "#     agg_df.columns = [col] + [col + '_' + c[0] +'_' + c[1] for c in agg_df.columns[1:]]\n",
    "    \n",
    "#     x_comb = pd.merge(left = x_comb, right= agg_df, how='left',\n",
    "#                     left_on=[col], right_on=[col])\n",
    "cat_count_var = []\n",
    "\n",
    "for i,col in tqdm(enumerate(new_cat_var)):\n",
    "    mean = x_comb['apache_4a_hospital_death_prob'].mean()\n",
    "    \n",
    "    #Compute the number of values and the mean of each group\n",
    "    agg = x_comb.groupby(col)['apache_4a_hospital_death_prob'].agg(['count', 'mean'])\n",
    "    counts = agg['count']\n",
    "    means =  agg['mean']\n",
    "    \n",
    "    #Compute the \"smoothed\" means\n",
    "    m= 3\n",
    "    smooth = (counts * means + m * mean) / (counts + m)\n",
    "    \n",
    "    #Final_val\n",
    "    x_comb[str(col)+'count_new'] = x_comb[col].map(smooth)\n",
    "    cat_count_var.append(str(col)+'count_new')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Remove Features with more than 70 percent missing values\n",
    "data_missing = (x_comb.isnull().sum() / len(x_comb)).sort_values(ascending = False)\n",
    "data_missing = data_missing.index[data_missing > 0.9]\n",
    "\n",
    "x_comb = x_comb.drop(columns = data_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_count_var_new = list(set(cat_count_var).intersection(x_comb.columns))\n",
    "#x_comb.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in tqdm(cat_count_var_new):\n",
    "    le = LabelEncoder()\n",
    "    x_comb[col] = le.fit_transform(x_comb[col].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mode(x):\n",
    "    return Counter(x).most_common(1)[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:00<00:00, 61711.19it/s]\n"
     ]
    }
   ],
   "source": [
    "aggs = {}\n",
    "\n",
    "for col in tqdm(new_cat_var):\n",
    "    aggs[col] = ['nunique','count',mode]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cat_var = list(set(new_cat_var).intersection(set(x_comb.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|▎         | 1/30 [00:06<03:07,  6.48s/it]\u001b[A\n",
      "  7%|▋         | 2/30 [00:13<03:05,  6.62s/it]\u001b[A\n",
      " 10%|█         | 3/30 [00:20<03:04,  6.84s/it]\u001b[A\n",
      " 13%|█▎        | 4/30 [00:28<03:03,  7.06s/it]\u001b[A\n",
      " 17%|█▋        | 5/30 [00:36<03:04,  7.40s/it]\u001b[A\n",
      " 20%|██        | 6/30 [00:45<03:06,  7.78s/it]\u001b[A\n",
      " 23%|██▎       | 7/30 [00:54<03:07,  8.14s/it]\u001b[A\n",
      " 27%|██▋       | 8/30 [01:04<03:10,  8.65s/it]\u001b[A\n",
      " 30%|███       | 9/30 [01:15<03:17,  9.39s/it]\u001b[A\n",
      " 33%|███▎      | 10/30 [01:26<03:18,  9.90s/it]\u001b[A\n",
      " 37%|███▋      | 11/30 [01:38<03:21, 10.62s/it]\u001b[A\n",
      " 40%|████      | 12/30 [01:50<03:20, 11.14s/it]\u001b[A\n",
      " 43%|████▎     | 13/30 [02:04<03:19, 11.73s/it]\u001b[A\n",
      " 47%|████▋     | 14/30 [02:17<03:17, 12.34s/it]\u001b[A\n",
      " 50%|█████     | 15/30 [02:32<03:15, 13.04s/it]\u001b[A\n",
      " 53%|█████▎    | 16/30 [02:48<03:14, 13.87s/it]\u001b[A\n",
      " 57%|█████▋    | 17/30 [03:06<03:16, 15.12s/it]\u001b[A\n",
      " 60%|██████    | 18/30 [03:23<03:07, 15.60s/it]\u001b[A\n",
      " 63%|██████▎   | 19/30 [03:39<02:53, 15.80s/it]\u001b[A\n",
      " 67%|██████▋   | 20/30 [03:56<02:43, 16.35s/it]\u001b[A\n",
      " 70%|███████   | 21/30 [04:15<02:34, 17.11s/it]\u001b[A\n",
      " 73%|███████▎  | 22/30 [04:35<02:23, 17.91s/it]\u001b[A\n",
      " 77%|███████▋  | 23/30 [04:55<02:09, 18.51s/it]\u001b[A\n",
      " 80%|████████  | 24/30 [05:15<01:54, 19.09s/it]\u001b[A\n",
      " 83%|████████▎ | 25/30 [05:37<01:38, 19.76s/it]\u001b[A\n",
      " 87%|████████▋ | 26/30 [05:59<01:21, 20.39s/it]\u001b[A\n",
      " 90%|█████████ | 27/30 [06:23<01:04, 21.53s/it]\u001b[A\n",
      " 93%|█████████▎| 28/30 [06:47<00:44, 22.48s/it]\u001b[A\n",
      " 97%|█████████▋| 29/30 [07:13<00:23, 23.42s/it]\u001b[A\n",
      "100%|██████████| 30/30 [07:40<00:00, 15.36s/it]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "for col in tqdm(new_cat_var):\n",
    "    \n",
    "    aggs_temp = aggs.copy()\n",
    "    aggs_temp.pop(col)\n",
    "    \n",
    "    agg_df = x_comb.groupby(col).agg(aggs_temp).reset_index()\n",
    "    agg_df.columns = [col] + [col + '_' + c[0] +'_' + c[1] for c in agg_df.columns[1:]]\n",
    "    \n",
    "    x_comb = pd.merge(left = x_comb, right= agg_df, how='left',\n",
    "                    left_on=[col], right_on=[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_comb['hospital_icu_id'] = x_comb['hospital_id'].astype(str) + '_' +  x_comb['icu_id'].astype(str)\n",
    "\n",
    "le = LabelEncoder()\n",
    "x_comb['hospital_icu_id'] = le.fit_transform(x_comb['hospital_icu_id'].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_comb = x_comb.drop(['encounter_id','patient_id'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(103023,)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = x_comb[0:91713]\n",
    "test_x =  x_comb[91713:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1_train, x1_val, y1_train, y1_val = train_test_split(train_x, y_train, \n",
    "                                                      test_size=0.2,\n",
    "                                                      random_state=0,\n",
    "                                                      stratify = y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42\n",
    "np.random.seed(random_state)\n",
    "\n",
    "lgb_params = {\n",
    "    'boost': 'gbdt',\n",
    "    'learning_rate': 0.01,\n",
    "    'max_depth':6,\n",
    "    'metric':{'auc'},\n",
    "    'num_threads': -1,\n",
    "    'objective': 'binary',\n",
    "    'verbosity': 1,\n",
    "    'num_leaves':15,\n",
    "    'bagging_fraction':0.5, #rows\n",
    "    'bagging_frequency':5,\n",
    "    'min_data_in_leaf':50,\n",
    "    'feature_fraction':0.2, # features\n",
    "    'lambda_l1':10,\n",
    "    'lambda_l2':15,\n",
    "    'bagging_seed':2019,\n",
    "    'min_gain_split':0.3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_data = lgb.Dataset(x1_train, label= y1_train)\n",
    "val_data = lgb.Dataset(x1_val,  label=  y1_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 1500 rounds.\n",
      "[200]\ttraining's auc: 0.896669\tvalid_1's auc: 0.886036\n",
      "[400]\ttraining's auc: 0.906778\tvalid_1's auc: 0.893552\n",
      "[600]\ttraining's auc: 0.913055\tvalid_1's auc: 0.897568\n",
      "[800]\ttraining's auc: 0.917567\tvalid_1's auc: 0.899968\n",
      "[1000]\ttraining's auc: 0.921141\tvalid_1's auc: 0.901554\n",
      "[1200]\ttraining's auc: 0.924223\tvalid_1's auc: 0.902784\n",
      "[1400]\ttraining's auc: 0.926952\tvalid_1's auc: 0.903727\n",
      "[1600]\ttraining's auc: 0.929499\tvalid_1's auc: 0.90442\n",
      "[1800]\ttraining's auc: 0.931761\tvalid_1's auc: 0.904922\n",
      "[2000]\ttraining's auc: 0.933889\tvalid_1's auc: 0.90541\n",
      "[2200]\ttraining's auc: 0.935883\tvalid_1's auc: 0.905704\n",
      "[2400]\ttraining's auc: 0.937784\tvalid_1's auc: 0.905953\n",
      "[2600]\ttraining's auc: 0.939521\tvalid_1's auc: 0.906172\n",
      "[2800]\ttraining's auc: 0.941179\tvalid_1's auc: 0.906382\n",
      "[3000]\ttraining's auc: 0.942724\tvalid_1's auc: 0.906483\n",
      "[3200]\ttraining's auc: 0.944305\tvalid_1's auc: 0.906672\n",
      "[3400]\ttraining's auc: 0.945841\tvalid_1's auc: 0.906837\n",
      "[3600]\ttraining's auc: 0.9473\tvalid_1's auc: 0.907036\n",
      "[3800]\ttraining's auc: 0.948682\tvalid_1's auc: 0.907067\n",
      "[4000]\ttraining's auc: 0.950044\tvalid_1's auc: 0.907157\n",
      "[4200]\ttraining's auc: 0.951376\tvalid_1's auc: 0.907226\n",
      "[4400]\ttraining's auc: 0.952668\tvalid_1's auc: 0.907309\n",
      "[4600]\ttraining's auc: 0.95396\tvalid_1's auc: 0.907406\n",
      "[4800]\ttraining's auc: 0.955164\tvalid_1's auc: 0.907465\n",
      "[5000]\ttraining's auc: 0.956337\tvalid_1's auc: 0.907557\n",
      "[5200]\ttraining's auc: 0.957527\tvalid_1's auc: 0.907653\n",
      "[5400]\ttraining's auc: 0.958647\tvalid_1's auc: 0.907679\n",
      "[5600]\ttraining's auc: 0.959723\tvalid_1's auc: 0.907714\n",
      "[5800]\ttraining's auc: 0.960773\tvalid_1's auc: 0.907732\n",
      "[6000]\ttraining's auc: 0.961833\tvalid_1's auc: 0.907795\n",
      "[6200]\ttraining's auc: 0.962807\tvalid_1's auc: 0.907769\n",
      "[6400]\ttraining's auc: 0.963766\tvalid_1's auc: 0.907822\n",
      "[6600]\ttraining's auc: 0.964721\tvalid_1's auc: 0.907859\n",
      "[6800]\ttraining's auc: 0.965666\tvalid_1's auc: 0.907861\n",
      "[7000]\ttraining's auc: 0.966588\tvalid_1's auc: 0.907848\n",
      "[7200]\ttraining's auc: 0.967444\tvalid_1's auc: 0.907855\n",
      "[7400]\ttraining's auc: 0.968274\tvalid_1's auc: 0.907848\n",
      "[7600]\ttraining's auc: 0.969114\tvalid_1's auc: 0.907847\n",
      "[7800]\ttraining's auc: 0.969949\tvalid_1's auc: 0.907864\n",
      "[8000]\ttraining's auc: 0.970751\tvalid_1's auc: 0.907923\n",
      "[8200]\ttraining's auc: 0.971557\tvalid_1's auc: 0.907858\n",
      "[8400]\ttraining's auc: 0.972301\tvalid_1's auc: 0.907866\n",
      "[8600]\ttraining's auc: 0.973026\tvalid_1's auc: 0.907884\n",
      "[8800]\ttraining's auc: 0.973772\tvalid_1's auc: 0.907882\n",
      "[9000]\ttraining's auc: 0.974452\tvalid_1's auc: 0.907878\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-96-3d3665591b51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m                     \u001b[0mvalid_sets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrn_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                     \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                     \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m                     )\n",
      "\u001b[0;32m~/anaconda2/envs/graph/lib/python3.6/site-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    216\u001b[0m                                     evaluation_result_list=None))\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/graph/lib/python3.6/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   1800\u001b[0m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[1;32m   1801\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1802\u001b[0;31m                 ctypes.byref(is_finished)))\n\u001b[0m\u001b[1;32m   1803\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__is_predicted_cur_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mFalse\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__num_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1804\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mis_finished\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lgb_clf = lgb.train(lgb_params,\n",
    "                    trn_data,\n",
    "                    13000,\n",
    "                    valid_sets = [trn_data, val_data],\n",
    "                    early_stopping_rounds=1500,\n",
    "                    verbose_eval=200\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = lgb_clf.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/s0c02nj/anaconda2/envs/graph/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "data_sub = x_test[['encounter_id']]\n",
    "data_sub['hospital_death'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sub.to_csv('/Users/s0c02nj/Desktop/OneDrive - Walmart Inc/WiDS/apache_imputed_23rdfeb_canbeoverfit.csv',index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_imp = pd.DataFrame(sorted(zip(lgb_clf.feature_importance(), \n",
    "                                      x_comb.columns), reverse=True), columns=['Value','Feature'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_unimp = list(feature_imp[feature_imp['Value'] == 0]['Feature'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(figsize=(12,30))\n",
    "# lgb.plot_importance(lgb_clf, max_num_features=130, height=0.8, ax=ax)\n",
    "# ax.grid(False)\n",
    "# plt.title(\"LightGBM - Feature Importance\", fontsize=12)\n",
    "# plt.show()\n",
    "# #data_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = StratifiedKFold(n_splits=10, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions = np.zeros(len(x_test))\n",
    "num_models = 0\n",
    "\n",
    "for train_index, valid_index in tqdm(kf.split(train_x, y_train)):\n",
    "    \n",
    "    d_train = lgb.Dataset(train_x.iloc[train_index], label=y_train[train_index])\n",
    "    d_val = lgb.Dataset(train_x.iloc[valid_index], label=y_train[valid_index])\n",
    "\n",
    "    clf = lgb.train(lgb_params, d_train, 20000, verbose_eval=1000, \n",
    "                    valid_sets = [d_train, d_val], early_stopping_rounds = 1500)\n",
    "    \n",
    "\n",
    "    \n",
    "    num_models += 1\n",
    "    \n",
    "    predictions += clf.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_10 = predictions /10.0\n",
    "data_sub = x_test[['encounter_id']]\n",
    "data_sub['hospital_death'] = predictions_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sub.to_csv('/Users/s0c02nj/Desktop/OneDrive - Walmart Inc/WiDS/best_till_now_imputing_apache_20thfeb.csv',index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature_imp = pd.DataFrame(sorted(zip(lgb_clf.feature_importance(), \n",
    "                                      x_comb.columns), reverse=True), columns=['Value','Feature'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature_imp[feature_imp['Feature'] == 'weightclass']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature_imp[feature_imp['Value'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 =  pd.read_csv('/Users/s0c02nj/Desktop/sub_10fold_keep_missing_as_it_is_18thFeb2020_newfeat.csv')\n",
    "a2 =  pd.read_csv('/Users/s0c02nj/Desktop/OneDrive - Walmart Inc/WiDS/ensemble_till_now_12thfeb.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a1 =  pd.read_csv('/Users/s0c02nj/Desktop/WiDS/ensemble4_lgbold_10thfeb_new_bmi_impute.csv')\n",
    "# a2 =  pd.read_csv('/Users/s0c02nj/Desktop/WiDS/sub_10fold_keep_missing_as_it_is_10thFeb2020_lgb_type4_ff_20_na_bmi.csv')\n",
    "# #a2 =  pd.read_csv('/Users/s0c02nj/Desktop/WiDS/sub_10fold_keep_missing_as_it_is_9thFeb2020_lgb_type3_ff_27.csv')\n",
    "# a3 =  pd.read_csv('/Users/s0c02nj/Desktop/WiDS/sub_type1_type2_ensemble_5thFeb2020.csv')\n",
    "# a4 =  pd.read_csv('/Users/s0c02nj/Desktop/WiDS/sub_10fold_keep_missing_as_it_is_11thFeb2020_lgb_type3_ff_27.csv')\n",
    "# a5 =  pd.read_csv('/Users/s0c02nj/Desktop/WiDS/pred_lgb_keeping_cat_cont_interchanged_11th_feb.csv')\n",
    "# a6 = pd.read_csv('/Users/s0c02nj/Desktop/WiDS/ensemble4tillnpw_11thfeb.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame()\n",
    "sub['encounter_id'] = a1['encounter_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ens = (a1['hospital_death'] + a2['hospital_death']\n",
    "           )/2.0\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub['hospital_death'] = pred_ens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv('/Users/s0c02nj/Desktop/OneDrive - Walmart Inc/WiDS/ensemble_till_now_18thfeb.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
