{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import metrics, preprocessing\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from catboost import Pool, CatBoostClassifier\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.layers import *\n",
    "from keras.layers import Dense,Flatten,Dropout,Input\n",
    "from keras.optimizers import RMSprop,Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing\n",
    "# train_data = pd.read_csv('/Users/s0c02nj/Desktop/WiDS/training_v2.csv')\n",
    "# test_data =  pd.read_csv('/Users/s0c02nj/Desktop/WiDS/unlabeled.csv')\n",
    "\n",
    "# sub_data = pd.read_csv('/Users/s0c02nj/Desktop/WiDS/samplesubmission.csv')\n",
    "# data_dict = pd.read_csv('/Users/s0c02nj/Desktop/WiDS/WiDS Datathon 2020 Dictionary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = np.load('/Users/s0c02nj/Downloads/train_features.npy')\n",
    "df_test = np.load('/Users/s0c02nj/Downloads/test_features.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame(df_train)\n",
    "df_test = pd.DataFrame(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(91713, 1536)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cat_var = list(data_dict[data_dict['Data Type'].isin(['binary','string'])]['Variable Name'])\n",
    "# cat_var.remove('icu_admit_type')\n",
    "# #cat_var.remove('bmi')\n",
    "# cat_var = cat_var[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cat_var  = [x for x in train_data.columns if train_data[x].dtype == 'object' ]\n",
    "# cont_var = [x for x in train_data.columns if train_data[x].dtype != 'object' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cont_var = list(data_dict[data_dict['Data Type'].isin(['integer','numeric'])]['Variable Name'])\n",
    "#cont_var.remove('pred')\n",
    "#cont_var = cont_var + ['bmi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train = train_data.drop('hospital_death',axis=1)\n",
    "y_train = train_data['hospital_death']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = test_data.drop('hospital_death',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_comb = pd.concat([x_train,x_test],sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_comb[\"apache_4a_hospital_death_prob\"] = x_comb[\"apache_4a_hospital_death_prob\"].replace(-1, np.nan)\n",
    "x_comb[\"apache_4a_icu_death_prob\"] = x_comb[\"apache_4a_icu_death_prob\"].replace(-1, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cat_var = list(set(cat_var).intersection(set(x_comb.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in tqdm(new_cat_var):\n",
    "    le = LabelEncoder()\n",
    "    x_comb[col] = le.fit_transform(x_comb[col].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cont_var = list(set(cont_var).intersection(set(x_comb.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in tqdm(new_cont_var):\n",
    "    x_comb[col] = x_comb[col].fillna(x_comb[col].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_cols = []\n",
    "\n",
    "for i,col in tqdm(enumerate(new_cat_var + new_cont_var)):\n",
    "    counter = Counter(x_comb[col])\n",
    "    x_comb[str(col)+'count'] = x_comb[col].apply(lambda x:counter[x])\n",
    "    count_cols.append(str(col)+'count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_cat_cols = new_cat_var + count_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_comb = x_comb.drop(['encounter_id','patient_id'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cont_var = list(set(cont_var).intersection(set(x_comb.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_rem = []\n",
    "for col in tqdm(new_cont_var):\n",
    "    if x_comb[col].std() <0.01 :\n",
    "        ls_rem.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_rem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_comb = x_comb.drop(['readmission_status'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cont_var.remove('readmission_status')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#total_cont = new_cont_var + count_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in tqdm(new_cont_var):\n",
    "    mu = x_comb[col].mean()\n",
    "    std = x_comb[col].std()\n",
    "    x_comb[col] = x_comb[col].apply(lambda x : (x-mu)/std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = x_comb[0:91713]\n",
    "test_x =  x_comb[91713:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cat = train_x[total_cat_cols]\n",
    "test_cat =  test_x[total_cat_cols]\n",
    "\n",
    "train_cont = train_x[new_cont_var]\n",
    "test_cont = test_x[new_cont_var]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(total_cat_cols) + len(new_cont_var)\n",
    "def get_train_test_data_cat(df):\n",
    "    \n",
    "    data=[]\n",
    "    for col in total_cat_cols:\n",
    "        data.append(df[col])\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_cat = get_train_test_data_cat(train_x)\n",
    "x_test_cat  = get_train_test_data_cat(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_comb = x_train_cat + [train_cont]\n",
    "x_test_comb =  x_test_cat +  [test_cont]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cat = to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(total_cat_cols) \n",
    "\n",
    "len(new_cont_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(total_cat_cols) + len(new_cont_var)\n",
    "def model_deep():\n",
    "    \n",
    "    layer_cat  = []\n",
    "    input_cat =  []\n",
    "    \n",
    "    #Categorical_var\n",
    "    for i,cat_var in tqdm(enumerate((total_cat_cols))): \n",
    "        \n",
    "        no_of_unique_cat  = x_comb[cat_var].nunique()\n",
    "        embedding_size = min(np.ceil((no_of_unique_cat)/2),30)\n",
    "        embedding_size = int(embedding_size)\n",
    "        \n",
    "        #Defining the input-----> branch_id\n",
    "        input_catg =  Input(shape=(1,))\n",
    "        layer_catg =  Embedding(no_of_unique_cat+1 ,embedding_size,input_length=1,trainable=True)(input_catg)\n",
    "        layer_catg =  Flatten()(layer_catg)\n",
    "        layer_cat.append(layer_catg)\n",
    "        input_cat.append(input_catg)\n",
    "    \n",
    "    \n",
    "    #continuous var\n",
    "    input_cont = Input(shape=(174,))\n",
    "    layer_cont = Dense(80, activation='relu')(input_cont)\n",
    "        \n",
    "    #Merging\n",
    "    layer_comb =  layer_cat + [layer_cont]\n",
    "    layer_comb = concatenate(layer_comb)\n",
    "    \n",
    "    #Layers\n",
    "    layer_dense = Dense(200, activation='relu')(layer_comb)\n",
    "    layer_dense = Dropout(0.5)(layer_dense)\n",
    "    \n",
    "    layer_dense = Dense(100, activation='relu')(layer_comb)\n",
    "    layer_dense = Dropout(0.5)(layer_dense)\n",
    "    \n",
    "    layer_dense = Dense(30, activation='relu')(layer_comb)\n",
    "    layer_dense = Dropout(0.5)(layer_dense)\n",
    "    \n",
    "    #Final output\n",
    "    layer_output = Dense(2,activation='softmax')(layer_dense)\n",
    "    \n",
    "    #Comb_inpus\n",
    "    input_comb = input_cat + [input_cont]\n",
    "    \n",
    "    #Final model\n",
    "    model = Model(inputs= input_comb ,outputs=layer_output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_model = model_deep()\n",
    "deep_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auc(y_true, y_pred):\n",
    "    def fallback_auc(y_true, y_pred):\n",
    "        try:\n",
    "            return metrics.roc_auc_score(y_true, y_pred)\n",
    "        except:\n",
    "            return 0.5\n",
    "    return tf.py_function(fallback_auc, (y_true, y_pred), tf.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_model.compile(loss = \"categorical_crossentropy\", optimizer = Adam(lr=0.0004), metrics = [auc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = deep_model.fit(x_train_comb,y_cat,\n",
    "                         batch_size = 512, \n",
    "                         epochs = 10, \n",
    "                         class_weight= [1,10],\n",
    "                         verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def model_deep():\n",
    "    \n",
    "#     #Defining the input-----> Transaction\n",
    "#     inputs1 = Input(shape=(x1_train.shape[1],))\n",
    "#     layer_dense = Dense(80, activation='relu')(inputs1)\n",
    "    \n",
    "#     layer_dense = Dense(50, activation='tanh')(layer_dense)\n",
    "#     #layer_dense = Dropout(0.3)(layer_dense)\n",
    "    \n",
    "#     layer_dense = Dense(25, activation='relu')(layer_dense)\n",
    "#     #layer_dense = Dropout(0.3)(layer_dense)\n",
    "    \n",
    "#     layer_dense = Dense(10, activation='tanh')(layer_dense)\n",
    "#     #layer_dense = Dropout(0.2)(layer_dense)\n",
    "    \n",
    "#     layer_dense = Dense(5, activation='relu')(layer_dense)\n",
    "#     #layer_dense = Dropout(0.2)(layer_dense)\n",
    "    \n",
    "#     probabilities = Dense(2,activation='softmax')(layer_dense)\n",
    "    \n",
    "#     model = Model(inputs= inputs1,outputs = probabilities)\n",
    "#     return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# history = model.fit(x1_train, y1_train,\n",
    "#                          batch_size = 512, \n",
    "#                          epochs = 10,\n",
    "#                          class_weight= [{0:1,1:10}],\n",
    "#                          validation_data = (x1_val,y1_val), \n",
    "#                          verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x1_train, x1_val, y1_train, y1_val = train_test_split(train_x, y_train, \n",
    "#                                                       test_size=0.2,\n",
    "#                                                       random_state=0,\n",
    "#                                                       stratify = y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_model(data, catcols,contcols):   \n",
    "    \n",
    "#     #Categorical Variable\n",
    "#     inputs_cat = []\n",
    "#     outputs_cat = []\n",
    "#     for c in catcols:\n",
    "#         num_unique_values = int(data[c].nunique())\n",
    "#         embed_dim = int(min(np.ceil((num_unique_values)/2), 50))\n",
    "#         inp = layers.Input(shape=(1,))\n",
    "#         out = layers.Embedding(num_unique_values + 1, embed_dim, name=c)(inp)\n",
    "#         out = layers.SpatialDropout1D(0.3)(out)\n",
    "#         out = layers.Reshape(target_shape=(embed_dim, ))(out)\n",
    "#         inputs_cat.append(inp)\n",
    "#         outputs_cat.append(out)\n",
    "    \n",
    "#     x_cat = layers.Concatenate()(outputs_cat)\n",
    "#     x_cat = layers.BatchNormalization()(x_cat)\n",
    "    \n",
    "#     x_cat = layers.Dense(200, activation=\"relu\")(x_cat)\n",
    "#     x_cat = layers.Dropout(0.3)(x_cat)\n",
    "#     x_cat = layers.BatchNormalization()(x_cat)\n",
    "    \n",
    "#     x_cat = layers.Dense(100, activation=\"relu\")(x_cat)\n",
    "#     x_cat = layers.Dropout(0.3)(x_cat)\n",
    "#     x_cat = layers.BatchNormalization()(x_cat)\n",
    "    \n",
    "    \n",
    "#     #Continuous Variable\n",
    "#     inputs_cont = layers.Input(shape=(data[contcols].shape[1],))\n",
    "#     x_cont = layers.Dense(200, activation=\"relu\")(inputs_cont)\n",
    "#     x_cont = layers.BatchNormalization()(x_cont)\n",
    "    \n",
    "#     #Merge,Cocatenating the inputes\n",
    "#     x_merge = layers.Concatenate()([x_cont, x_cat])\n",
    "#     x_merge = layers.Dense(200, activation=\"relu\")(x_merge)\n",
    "#     x_merge = layers.Dropout(0.4)(x_merge)\n",
    "#     x_merge = layers.BatchNormalization()(x_merge)\n",
    "    \n",
    "#     #Output\n",
    "#     y = layers.Dense(1, activation=\"softmax\")(x_merge)\n",
    "    \n",
    "#     #Comb_inpus\n",
    "#     inputs_comb = inputs_cat + [inputs_cont]\n",
    "\n",
    "#     #Final model\n",
    "#     model = Model(inputs= inputs_comb ,outputs = y)\n",
    "    \n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deep_model = create_model(x_comb, new_cat_var,new_cont_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deep_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def auc(y_true, y_pred):\n",
    "#     def fallback_auc(y_true, y_pred):\n",
    "#         try:\n",
    "#             return metrics.roc_auc_score(y_true, y_pred)\n",
    "#         except:\n",
    "#             return 0.5\n",
    "#     return tf.py_function(fallback_auc, (y_true, y_pred), tf.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deep_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[auc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = deep_model.fit(x_train_comb,y_train,\n",
    "#                          batch_size = 512, \n",
    "#                          epochs = 100, \n",
    "#                          validation_split = 0.2, \n",
    "#                          class_weight= [1,10],\n",
    "#                          verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
