{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Input, Dense, Lambda, Flatten, Reshape, Concatenate,Activation\n",
    "from keras.layers import concatenate\n",
    "from keras.layers import Conv2D, Conv2DTranspose, ZeroPadding2D,MaxPooling2D, Cropping2D, BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras import metrics\n",
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "from keras import losses\n",
    "from keras.utils import plot_model\n",
    "from keras.callbacks import Callback,ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import skimage.io as io\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import datetime\n",
    "\n",
    "import cv2\n",
    "%matplotlib inline\n",
    "#import pydot\n",
    "from PIL import Image\n",
    "import glob\n",
    "import os\n",
    "#from pyemd import emd, emd_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters initialization\n",
    "nb_rows = 224   # X dimension of the image\n",
    "nb_cols = 224   # Y dimesnion of the image\n",
    "#total_frames = 30\n",
    "\n",
    "nb_channel = 3 # numbe rof channels in images 3 for color(RGB) and 1 for Gray\n",
    "\n",
    "BS = 10\n",
    "batch_size = BS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.transform import rescale\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    rotation_range = 0,\n",
    "    width_shift_range = 0,\n",
    "    height_shift_range = 0,\n",
    "    vertical_flip = False,)\n",
    "\n",
    "\n",
    "# batch_labels is the one hot representation of the output\n",
    "def initialize_batch_data(batch_size):\n",
    "    batch_data = np.zeros((batch_size, nb_rows, nb_cols, nb_channel)) \n",
    "    batch_labels = np.zeros((batch_size,nb_rows, nb_cols,1)) \n",
    "    return batch_data, batch_labels\n",
    "\n",
    "# Image Croping\n",
    "# def crop(image):\n",
    "#     if image.shape[0] != image.shape[1]:\n",
    "#         return image[0:120,20:140]\n",
    "#     else:\n",
    "#         return image\n",
    "\n",
    "# Resizing the image based on dimension\n",
    "def resize(image):\n",
    "    return  cv2.resize(image, (nb_rows,nb_cols), interpolation = cv2.INTER_AREA)\n",
    "\n",
    "\n",
    "def preprocess_img(img, mode):\n",
    "    img = (img - img.min())/(img.max() - img.min())\n",
    "    if mode == 'train':\n",
    "        if np.random.randn() > 0:\n",
    "            img = datagen.random_transform(img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(mode,source_path):\n",
    "    image_list=[]\n",
    "    for filename in glob.glob(str(source_path)+'images/*'):\n",
    "        path1 = os.path.basename(filename)\n",
    "        image_list.append(path1)\n",
    "#     paths = image_list[0:100]\n",
    "    return image_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enahancing the sal_map images/ manual thresholding\n",
    "\n",
    "def getBatchImage(folder_list,indexes, mode,source_path):\n",
    "    list_IDs_temp = [folder_list[k] for k in indexes]\n",
    "#     print(\"indexes\",list_IDs_temp)\n",
    "    X=[]\n",
    "    Y=[]\n",
    "    for index, h in enumerate(list_IDs_temp):\n",
    "        I = io.imread(str(source_path)+\"images/{}\".format(h))\n",
    "        resized = resize(I)\n",
    "        new_image = preprocess_img(resized, mode)\n",
    "        X.append(new_image)\n",
    "        bin_label = np.zeros((224,224))\n",
    "        label = io.imread(str(source_path)+\"salMap/{}\".format(h))\n",
    "        resized = resize(label)\n",
    "        labels = preprocess_img(resized, mode)\n",
    "        for i in range(0,224):\n",
    "            for j in range(0,224):\n",
    "                if labels[i][j]<26:\n",
    "                    bin_label[i][j] = 0\n",
    "                elif labels[i][j]<51:\n",
    "                    bin_label[i][j] = 0.111\n",
    "                elif labels[i][j]<76:\n",
    "                    bin_label[i][j] = 0.222\n",
    "                elif labels[i][j]<102:\n",
    "                    bin_label[i][j] = 0.333\n",
    "                elif labels[i][j]<128:\n",
    "                    bin_label[i][j] = 0.444\n",
    "                elif labels[i][j]<154:\n",
    "                    bin_label[i][j] = 0.556\n",
    "                elif labels[i][j]<180:\n",
    "                    bin_label[i][j] = 0.667\n",
    "                elif labels[i][j]<206:\n",
    "                    bin_label[i][j] = 0.778\n",
    "                elif labels[i][j]<230:\n",
    "                    bin_label[i][j] = 0.889                  \n",
    "                else:\n",
    "                    bin_label[i][j] = 1\n",
    "        Y.append(bin_label)\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "    Y=Y[:,:,:,np.newaxis]\n",
    "    return X,[Y,Y,Y,Y]\n",
    "\n",
    "def get_newBatch(folder_list,batch_size,index):\n",
    "    indexes = np.arange(len(folder_list))\n",
    "    np.random.shuffle(indexes)\n",
    "    indexes = indexes[index*batch_size:(index+1)*batch_size]\n",
    "    return indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generator function\n",
    "def generator(batch_size,source_path,mode='train'):\n",
    "    \n",
    "    while True:\n",
    "        folder_list = getData(mode,source_path)\n",
    "\n",
    "        num_batches = len(folder_list)//batch_size # calculate the number of batches\n",
    "        print('No of Batch : ', num_batches,' mode: ' , mode)\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            # you yield the batch_data and the batch_labels, remember what does yield do\n",
    "            indexes = get_newBatch(folder_list,batch_size,batch)\n",
    "            yield getBatchImage(folder_list,indexes,mode,source_path)\n",
    "\n",
    "        # Code for the remaining data points which are left after full batches\n",
    "        if (len(folder_list) != batch_size*num_batches):\n",
    "            batch_size = len(folder_list) - (batch_size*num_batches)\n",
    "            indexes = get_newBatch(folder_list,batch_size,batch)\n",
    "            yield getBatchImage(folder_list,indexes,mode,source_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BS=20\n",
    "# batch_size = BS\n",
    "training_Path = \"/Users/s0c02nj/Downloads/lagdata/train/\"\n",
    "validation_Path = \"/Users/s0c02nj/Downloads/lagdata/validation/\"\n",
    "# train_generator = generator(batch_size, training_Path, mode='train')\n",
    "# validation_generator = generator(batch_size, validation_Path, mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for t in validation_generator:\n",
    "#     print(t[0].shape,t[1][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "\n",
    "def get_step_per_epoch(num_train_sequences, b_size):\n",
    "    if (num_train_sequences%b_size) == 0:\n",
    "        steps_per_epoch = int(num_train_sequences/b_size)\n",
    "    else:\n",
    "        steps_per_epoch = (num_train_sequences//b_size) + 1\n",
    "    return steps_per_epoch\n",
    "\n",
    "def get_validation_steps(num_val_sequences, b_size):\n",
    "    if (num_val_sequences%b_size) == 0:\n",
    "        validation_steps = int(num_val_sequences/b_size)\n",
    "    else:\n",
    "        validation_steps = (num_val_sequences//b_size) + 1\n",
    "    return validation_steps\n",
    "    \n",
    "\n",
    "def callbacks_list(model_name, factor_rate, epoch_patience):\n",
    "    print('factor_rate: ',factor_rate)\n",
    "    model_name = model_name + '_'+'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
    "    if not os.path.exists(model_name):\n",
    "        os.mkdir(model_name)\n",
    "    filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{bafc_accuracy:.5f}-{val_loss:.5f}-{val_bafc_accuracy:.5f}.h5'\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_bafc_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', period=1)\n",
    "    LR = ReduceLROnPlateau(monitor='val_bafc_loss', factor=factor_rate, patience=epoch_patience, cooldown=1, verbose=1, mode='auto', min_delta=0.0001) # write the REducelronplateau code here\n",
    "    callbacks_list = [checkpoint, LR]   \n",
    "    return callbacks_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionModel:\n",
    "  def __init__(self):\n",
    "        self.batch_size = BS\n",
    "        self.epochs = 5 #10\n",
    "        self.lr = 0.001\n",
    "\n",
    "        \n",
    "  #Defines the model architecture      \n",
    "  def DeepAttentionModel(self):\n",
    "    img_rows, img_cols, img_chns = 224, 224, 3\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        original_img_size = (img_chns, img_rows, img_cols)\n",
    "    else:\n",
    "        original_img_size = (img_rows, img_cols, img_chns)\n",
    "    \n",
    "    #print(\"###########################################\")\n",
    "    #print(\"original_img_size \", original_img_size)\n",
    "    #print(\"###########################################\")\n",
    "    \n",
    "    self.x = Input(shape=original_img_size)\n",
    "    padded_x = ZeroPadding2D(padding=(35), data_format=\"channels_last\")(self.x)\n",
    "    \n",
    "    \n",
    "    #'Encoder'\n",
    "    conv_1_1 = Conv2D(64, kernel_size=(3, 3),strides=(1, 1), padding='valid', activation='relu',data_format=\"channels_last\")(padded_x)\n",
    "    padded_conv_1_1 = ZeroPadding2D(padding=(1), data_format=\"channels_last\")(conv_1_1)\n",
    "    conv_1_2 = Conv2D(64, kernel_size=(3, 3),strides=(1, 1), padding='valid', activation='relu',data_format=\"channels_last\")(padded_conv_1_1)\n",
    "    pool_1 = MaxPooling2D(pool_size=(2, 2), strides=(2,2), padding='same', data_format=\"channels_last\")(conv_1_2)\n",
    "    padded_input_pool_1=  ZeroPadding2D(padding=(1), data_format=\"channels_last\")(pool_1)\n",
    "    \n",
    "    conv_2_1 = Conv2D(128, kernel_size=(3, 3),strides=(1, 1), padding='valid', activation='relu',data_format=\"channels_last\")(padded_input_pool_1)\n",
    "    padded_input_conv_2_1 = ZeroPadding2D(padding=(1), data_format=\"channels_last\")(conv_2_1)\n",
    "    conv_2_2 = Conv2D(128, kernel_size=(3, 3),strides=(1, 1), padding='valid', activation='relu',data_format=\"channels_last\")(padded_input_conv_2_1)\n",
    "    pool_2 = MaxPooling2D(pool_size=(2, 2), strides=(2,2), padding='same', data_format=\"channels_last\")(conv_2_2)\n",
    "    padded_input_pool_2=  ZeroPadding2D(padding=(1), data_format=None)(pool_2)\n",
    "    \n",
    "    conv_3_1 = Conv2D(256, kernel_size=(3, 3),strides=(1, 1), padding='valid', activation='relu')(padded_input_pool_2)\n",
    "    padded_input_conv_3_1 = ZeroPadding2D(padding=(1), data_format=None)(conv_3_1)\n",
    "    conv_3_2 = Conv2D(256, kernel_size=(3, 3),strides=(1, 1), padding='valid', activation='relu')(padded_input_conv_3_1)\n",
    "    padded_input_conv_3_2 = ZeroPadding2D(padding=(1), data_format=None)(conv_3_2)\n",
    "    conv_3_3 = Conv2D(256, kernel_size=(3, 3),strides=(1, 1), padding='valid', activation='relu')(padded_input_conv_3_2)\n",
    "    pool_3 = MaxPooling2D(pool_size=(2, 2), strides=(2,2), padding='same', data_format=None)(conv_3_3)\n",
    "    padded_input_pool_3 =  ZeroPadding2D(padding=(1), data_format=None)(pool_3)\n",
    "    \n",
    "    conv_4_1 = Conv2D(512, kernel_size=(3, 3),strides=(1, 1), padding='valid', activation='relu')(padded_input_pool_3)\n",
    "    padded_input_conv_4_1 = ZeroPadding2D(padding=(1), data_format=None)(conv_4_1)\n",
    "    conv_4_2 = Conv2D(512, kernel_size=(3, 3),strides=(1, 1), padding='valid', activation='relu')(padded_input_conv_4_1)\n",
    "    padded_input_conv_4_2 = ZeroPadding2D(padding=(1), data_format=None)(conv_4_2)\n",
    "    conv_4_3 = Conv2D(512, kernel_size=(3, 3),strides=(1, 1), padding='valid', activation='relu')(padded_input_conv_4_2)\n",
    "    pool_4 = MaxPooling2D(pool_size=(2, 2), strides=(2,2), padding='same', data_format=None)(conv_4_3)\n",
    "    padded_input_pool_4 =  ZeroPadding2D(padding=(1), data_format=None)(pool_4)\n",
    "    \n",
    "    conv_5_1 = Conv2D(512, kernel_size=(3, 3),strides=(1, 1), padding='valid', activation='relu')(padded_input_pool_4)\n",
    "    padded_input_conv_5_1 = ZeroPadding2D(padding=(1), data_format=None)(conv_5_1)\n",
    "    conv_5_2 = Conv2D(512, kernel_size=(3, 3),strides=(1, 1), padding='valid', activation='relu')(padded_input_conv_5_1)\n",
    "    padded_input_conv_5_2 = ZeroPadding2D(padding=(1), data_format=None)(conv_5_2)\n",
    "    conv_5_3 = Conv2D(512, kernel_size=(3, 3),strides=(1, 1), padding='valid', activation='relu')(padded_input_conv_5_2)\n",
    "    \n",
    "    \n",
    "    #'Decoder'\n",
    "    deconv_5_1 = Conv2DTranspose(512,kernel_size=(4, 4),strides=(2, 2), padding='valid', activation='relu')(conv_5_3)\n",
    "    deconv_5_2 = Conv2DTranspose(256,kernel_size=(4, 4),strides=(2, 2), padding='valid', activation='relu')(deconv_5_1)\n",
    "    deconv_5_3 = Conv2DTranspose(128,kernel_size=(4, 4),strides=(2, 2), padding='valid', activation='relu')(deconv_5_2)\n",
    "    deconv_5_4 = Conv2DTranspose(64,kernel_size=(4, 4),strides=(2, 2), padding='valid', activation='relu')(deconv_5_3)\n",
    "    \n",
    "    attention1 = Conv2D(1, kernel_size=(3, 3),strides=(1, 1), padding='valid', activation='relu')(deconv_5_4 )\n",
    "    attention1c = Cropping2D(cropping= ((54,54),(54,54)), data_format=None)(attention1)\n",
    "    self.bn_attention1c = BatchNormalization(name='ba1c')(attention1c)\n",
    "    \n",
    "    deconv_4_1 = Conv2DTranspose(256,kernel_size=(4, 4),strides=(2, 2), padding='valid', activation='relu')(conv_4_3)\n",
    "    deconv_4_2 = Conv2DTranspose(128,kernel_size=(4, 4),strides=(2, 2), padding='valid', activation='relu')(deconv_4_1)\n",
    "    deconv_4_3 = Conv2DTranspose(64,kernel_size=(4, 4),strides=(2, 2), padding='valid', activation='relu')(deconv_4_2)\n",
    "    \n",
    "    attention2 = Conv2D(1, kernel_size=(3, 3),strides=(1, 1), padding='valid', activation='relu')(deconv_4_3 )\n",
    "    attention2c = Cropping2D(cropping= ((42,42),(42,42)))(attention2)\n",
    "    self.bn_attention2c = BatchNormalization(name='ba2c')(attention2c)\n",
    "    \n",
    "    deconv_3_1 = Conv2DTranspose(128,kernel_size=(4, 4),strides=(2, 2), padding='valid', activation='relu')(conv_3_3)\n",
    "    deconv_3_2 = Conv2DTranspose(64,kernel_size=(4, 4),strides=(2, 2), padding='valid', activation='relu')(deconv_3_1)\n",
    "    \n",
    "    attention3 = Conv2D(1, kernel_size=(3, 3),strides=(1, 1), padding='valid', activation='relu')(deconv_3_2)\n",
    "    attention3c = Cropping2D(cropping= ((36,36),(36,36)))(attention3)\n",
    "    self.bn_attention3c = BatchNormalization(name='ba3c')(attention3c)\n",
    "    \n",
    "    attention = concatenate([attention1c,attention2c,attention3c])\n",
    "    padded_attention = ZeroPadding2D(padding=(1))(attention)\n",
    "    final_attention =  Conv2D(1, kernel_size=(3, 3),strides=(1, 1), padding='valid', activation='relu',data_format=\"channels_last\")(padded_attention)\n",
    "    self.bn_final_attention = BatchNormalization(name='bafc')(final_attention)\n",
    "    \n",
    "    \n",
    "    self.model = Model(inputs=self.x, outputs=[self.bn_attention1c,self.bn_attention2c,self.bn_attention3c,self.bn_final_attention ])\n",
    "    \n",
    "    \n",
    "    def custom_loss(y_true, y_pred):\n",
    "        \n",
    "        loss1 = losses.binary_crossentropy(y_true,self.bn_attention1c)\n",
    "        loss2 = losses.binary_crossentropy(y_true,self.bn_attention2c)\n",
    "        loss3 = losses.binary_crossentropy(y_true,self.bn_attention3c)\n",
    "        loss4 = losses.binary_crossentropy(y_true,self.bn_final_attention)\n",
    "\n",
    "        return (loss1+loss2+loss3+loss4)/4.0\n",
    "\n",
    "    sgd = optimizers.SGD(lr=self.lr) #Stochastic Gradient Descent Optimizer\n",
    "    self.loss = custom_loss\n",
    "    self.model.compile(optimizer = sgd , loss = self.loss, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "  def Train(self):\n",
    "    global output\n",
    "    global input_image_full\n",
    "    global req_output_image_full\n",
    "    factor=0.20\n",
    "    patience=5\n",
    "    model_name_folder = 'conv2d_attention'\n",
    "    \n",
    "    num_train_sequences = len(getData('train',training_Path))\n",
    "    num_val_sequences = len(getData('val',validation_Path))\n",
    "    \n",
    "    steps_per_epoch = get_step_per_epoch(num_train_sequences, batch_size)\n",
    "    validation_steps = get_validation_steps(num_val_sequences, batch_size)\n",
    "    \n",
    "    train_generator = generator(batch_size, training_Path, mode='train')\n",
    "    validation_generator = generator(batch_size, validation_Path, mode='val')\n",
    "    \n",
    "    callbacks_model = callbacks_list(model_name_folder,factor,patience)\n",
    "    \n",
    "#     X_train,Y_train = training_generator[0]\n",
    "#     X_test,Y_test = validation_generator[0]\n",
    "    # History object stores loss and accuracy\n",
    "    hist_obj = self.model.fit_generator(train_generator,validation_data=validation_generator,\n",
    "                                        steps_per_epoch=steps_per_epoch,validation_steps=validation_steps,\n",
    "                                        epochs=self.epochs,verbose=1,callbacks=callbacks_model,initial_epoch=0)\n",
    "    \n",
    "#     ,callbacks=callbacks_list_for_Conv2D_plus_lstm_model\n",
    "#     output = self.model.predict(X_test)\n",
    "#     output = np.array(output)\n",
    "    \n",
    "    # Making data global, to be used for visualization\n",
    "#     input_image_full = X_train[:,:,:]\n",
    "#     req_output_image_full =Y_train[0][:,:,:]\n",
    "    \n",
    "    return hist_obj\n",
    " \n",
    "  \n",
    "#   # Prints the model architecture\n",
    "  def get_Model_Summary(self):      \n",
    "      print(self.model.summary())\n",
    "    \n",
    "\n",
    "#   def Visualise_Output(self,idx):\n",
    "#     self.input_image =input_image_full[idx,:,:]\n",
    "#     self.req_output_image =req_output_image_full[idx,:,:]\n",
    "#     pred_image_1 = output[0,idx,:,:,:]\n",
    "#     pred_image_2 = output[1,idx,:,:,:]\n",
    "#     pred_image_3 = output[2,idx,:,:,:]\n",
    "#     pred_image_4 = output[3,idx,:,:,:]\n",
    "#     fig = plt.figure(figsize=(20,10))\n",
    "#     plt.subplot(231)\n",
    "#     plt.imshow((self.input_image.reshape(224,224,3)), interpolation='none')\n",
    "#     plt.title(\"Input - \")\n",
    "#     ax = plt.gca()\n",
    "#     ax.grid(False)\n",
    "#     plt.subplot(232)\n",
    "#     plt.imshow(self.req_output_image.reshape(224,224)*255,interpolation='none')\n",
    "#     plt.title(\"Ground Truth Attention\")\n",
    "#     ax = plt.gca()\n",
    "#     ax.grid(False)\n",
    "#     plt.subplot(233)\n",
    "#     plt.imshow(pred_image_1.reshape(224,224)*255,interpolation='none')\n",
    "#     plt.title(\"Predicted Attention 1 \")\n",
    "#     ax = plt.gca()\n",
    "#     ax.grid(False)\n",
    "#     plt.subplot(234)\n",
    "#     plt.imshow(pred_image_2.reshape(224,224)*255,interpolation='none')\n",
    "#     plt.title(\"Predicted Attention 2 \")\n",
    "#     ax = plt.gca()\n",
    "#     ax.grid(False)\n",
    "#     plt.subplot(235)\n",
    "#     plt.imshow(pred_image_3.reshape(224,224)*255,interpolation='none')\n",
    "#     plt.title(\"Predicted Attention 3 \")\n",
    "#     ax = plt.gca()\n",
    "#     ax.grid(False)\n",
    "#     plt.subplot(236)\n",
    "#     plt.imshow(pred_image_4.reshape(224,224)*255,interpolation='none')\n",
    "#     plt.title(\"Predicted Final Attention \")\n",
    "#     ax = plt.gca()\n",
    "#     ax.grid(False)\n",
    "#     plot_name = 'plot' + '_'+str(idx)+'.png'\n",
    "#     print(plot_name)\n",
    "#     fig.savefig(plot_name)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deconvNet = AttentionModel()\n",
    "deconvNet.DeepAttentionModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deconvNet.get_Model_Summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hist_obj = deconvNet.Train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
