{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8hk2YZ5wXey9"
   },
   "source": [
    "### Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch==1.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchvision==0.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i_3MlXOFINOF"
   },
   "outputs": [],
   "source": [
    "# %reload_ext autoreload\n",
    "# %autoreload 2\n",
    "# %matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, sampler\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, datasets, models\n",
    "import random\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "import math\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-MhlChXPBYnD"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "#from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 222\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_image(breed, dog, data_dir):\n",
    "    \n",
    "    img = plt.imread(data_dir + 'images/Images/' + breed + '/' + dog + '.jpg')\n",
    "    tree = ET.parse(data_dir + 'annotations/Annotation/' + breed + '/' + dog)\n",
    "    xmin = int(tree.getroot().findall('object')[0].find('bndbox').find('xmin').text)\n",
    "    xmax = int(tree.getroot().findall('object')[0].find('bndbox').find('xmax').text)\n",
    "    ymin = int(tree.getroot().findall('object')[0].find('bndbox').find('ymin').text)\n",
    "    ymax = int(tree.getroot().findall('object')[0].find('bndbox').find('ymax').text)\n",
    "    img = img[ymin:ymax, xmin:xmax, :]\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/kaggle/input/stanford-dogs-dataset/'\n",
    "breed_list = os.listdir(data_dir + 'images/Images/')\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "\n",
    "for i in range(4):\n",
    "    \n",
    "    plt.subplot(421 + (i*2))\n",
    "    \n",
    "    breed = np.random.choice(breed_list)\n",
    "    dog = np.random.choice(os.listdir(data_dir + 'annotations/Annotation/' + breed))\n",
    "    img = plt.imread(data_dir + 'images/Images/' + breed + '/' + dog + '.jpg')\n",
    "    plt.imshow(img)\n",
    "    \n",
    "    tree = ET.parse(data_dir + 'annotations/Annotation/' + breed + '/' + dog)\n",
    "    xmin = int(tree.getroot().findall('object')[0].find('bndbox').find('xmin').text)\n",
    "    xmax = int(tree.getroot().findall('object')[0].find('bndbox').find('xmax').text)\n",
    "    ymin = int(tree.getroot().findall('object')[0].find('bndbox').find('ymin').text)\n",
    "    ymax = int(tree.getroot().findall('object')[0].find('bndbox').find('ymax').text)\n",
    "    \n",
    "    plt.plot([xmin, xmax, xmax, xmin, xmin], [ymin, ymin, ymax, ymax, ymin])\n",
    "    \n",
    "    crop_img = crop_image(breed, dog, data_dir)\n",
    "    print(crop_img.shape)\n",
    "    plt.subplot(422 + (i*2))\n",
    "    plt.imshow(crop_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'cropped_data' not in os.listdir():\n",
    "    \n",
    "    os.mkdir('cropped_data')\n",
    "    \n",
    "    for breed in breed_list:\n",
    "        os.mkdir('cropped_data/' + breed)\n",
    "    \n",
    "print('Created {} folders to store cropped images of the different breeds.'.format(len(os.listdir('cropped_data'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'cropped_data_test' not in os.listdir():\n",
    "    \n",
    "    os.mkdir('cropped_data_test')\n",
    "    \n",
    "    for breed in breed_list:\n",
    "        os.mkdir('cropped_data_test/' + breed)\n",
    "    \n",
    "print('Created {} folders to store cropped images of the different breeds.'.format(len(os.listdir('cropped_data_test'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "test_mat = scipy.io.loadmat('/kaggle/input/test-list2/test_list.mat')\n",
    "\n",
    "test_files = set()\n",
    "\n",
    "for i in range(test_mat[\"file_list\"].shape[0]):\n",
    "    test_files.add(test_mat[\"file_list\"][i][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_count = 0\n",
    "for breed in tqdm(os.listdir('cropped_data')):    \n",
    "    for file in os.listdir(data_dir + 'annotations/Annotation/' + breed):\n",
    "                \n",
    "        img = Image.open(data_dir + 'images/Images/' + breed + '/' + file + '.jpg')\n",
    "        tree = ET.parse(data_dir + 'annotations/Annotation/' + breed + '/' + file)\n",
    "        xmin = int(tree.getroot().findall('object')[0].find('bndbox').find('xmin').text)\n",
    "        xmax = int(tree.getroot().findall('object')[0].find('bndbox').find('xmax').text)\n",
    "        ymin = int(tree.getroot().findall('object')[0].find('bndbox').find('ymin').text)\n",
    "        ymax = int(tree.getroot().findall('object')[0].find('bndbox').find('ymax').text)\n",
    "        img = img.crop((xmin,ymin,xmax,ymax))\n",
    "        img = img.convert('RGB')\n",
    "        \n",
    "        if breed+\"/\"+file+\".jpg\" in test_files:\n",
    "            test_count += 1\n",
    "            img.save('cropped_data_test/' + breed + '/' + file + '.jpg')\n",
    "        else:\n",
    "            img.save('cropped_data/' + breed + '/' + file + '.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_count = 0\n",
    "file_name = \"cropped_data_test/\"\n",
    "for folder in os.listdir(file_name):\n",
    "    \n",
    "    for _ in os.listdir(file_name + folder):    \n",
    "        img_count += 1\n",
    "    \n",
    "print('No. of Images: {}'.format(img_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P-sdyCuSIWoy"
   },
   "outputs": [],
   "source": [
    "# Data Augmentation\n",
    "batch_size = 128\n",
    "image_size = 224\n",
    "\n",
    "# image_transforms = {\n",
    "    \n",
    "#     'train':torchvision.transforms.Compose([\n",
    "#             torchvision.transforms.Resize(size=(image_size, image_size)),\n",
    "#             torchvision.transforms.RandomHorizontalFlip(),\n",
    "#             torchvision.transforms.ToTensor(),\n",
    "#             torchvision.transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "#                                              std=(0.229, 0.224, 0.225))\n",
    "#     ]),\n",
    "#     'val':torchvision.transforms.Compose([\n",
    "#             torchvision.transforms.Resize(size=(image_size, image_size)),\n",
    "#             torchvision.transforms.ToTensor(),\n",
    "#             torchvision.transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "#                                              std=(0.229, 0.224, 0.225))\n",
    "#         ])\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transforms = {\n",
    "    # Train uses data augmentation\n",
    "    'train':\n",
    "    transforms.Compose([\n",
    "        transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),\n",
    "        transforms.RandomRotation(degrees=15),\n",
    "        transforms.ColorJitter(),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.CenterCrop(size=224),  # Image net standards\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])  # Imagenet standards\n",
    "    ]),\n",
    "    # Validation does not use augmentation\n",
    "    'val':\n",
    "    transforms.Compose([\n",
    "        transforms.Resize(size=256),\n",
    "        transforms.CenterCrop(size=224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform_pipe = torchvision.transforms.Compose([\n",
    "#     torchvision.transforms.ToPILImage(), # Convert np array to PILImage\n",
    "    \n",
    "#     # Resize image to 224 x 224 as required by most vision models\n",
    "#     torchvision.transforms.Resize(\n",
    "#         size=(224, 224)\n",
    "#     ),\n",
    "    \n",
    "#     # Convert PIL image to tensor with image values in [0, 1]\n",
    "#     torchvision.transforms.ToTensor(),\n",
    "    \n",
    "#     torchvision.transforms.Normalize(\n",
    "#         mean=[0.485, 0.456, 0.406],\n",
    "#         std=[0.229, 0.224, 0.225]\n",
    "#     )\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.ImageFolder(root='cropped_data')\n",
    "test_dataset = datasets.ImageFolder(root='cropped_data_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset.classes, train_dataset.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.transform = image_transforms['train']\n",
    "test_dataset.transform = image_transforms['val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainiter = iter(train_loader)\n",
    "features, labels = next(trainiter)\n",
    "print(features.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import FloatTensor\n",
    "\n",
    "def new_parameter(*size):\n",
    "    out = nn.Parameter(FloatTensor(*size), requires_grad=True)\n",
    "    torch.nn.init.xavier_normal_(out)\n",
    "    return out\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \n",
    "    def __init__(self, attention_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attention = new_parameter(attention_size, 1)\n",
    "\n",
    "    def forward(self, x_in):\n",
    "        # after this, we have (bs, feature_size, feature_size) with a diff weight per each cell\n",
    "        attention_score = torch.matmul(x_in, self.attention).squeeze()\n",
    "        attention_score = F.softmax(attention_score, dim=1).view(x_in.size(0), x_in.size(1), 1)\n",
    "        \n",
    "        return attention_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = 2048\n",
    "fmap_size = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resnet = models.resnet50()\n",
    "# resnet.load_state_dict(torch.load('imagenet_models/resnet50-19c8e357.pth'), strict=False)\n",
    "\n",
    "# # freezing parameters\n",
    "# for param in resnet.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# layers = list(models.resnet50().children())[:-2]\n",
    "# resnet = nn.Sequential(*layers).cuda()\n",
    "\n",
    "# x = resnet(torch.randn(1, 3, image_size, image_size).cuda())\n",
    "# x.shape\n",
    "\n",
    "# N=2\n",
    "\n",
    "# attention_row = Attention(features).cuda()\n",
    "# attention_col = Attention(features).cuda()\n",
    "\n",
    "\n",
    "# x = resnet(torch.randn(N, 3, image_size, image_size).cuda())\n",
    "\n",
    "# x = x.view(N, fmap_size ** 2, features)\n",
    "# print(x.shape)\n",
    "\n",
    "\n",
    "# x = torch.bmm(torch.transpose(x, 1, 2), x)/ (fmap_size ** 2) \n",
    "# x = torch.sqrt(x + 1e-5)\n",
    "\n",
    "# print(x.shape)\n",
    "\n",
    "# y = attention_row(x)\n",
    "\n",
    "# z = attention_col(x.permute(0, 2, 1))\n",
    "\n",
    "# scored_y = x * y\n",
    "\n",
    "# # now, sum across dim 1 to get the expected feature vector\n",
    "# condensed_y = torch.sum(scored_y, dim=1)\n",
    "\n",
    "# scored_z = x * z\n",
    "\n",
    "# # now, sum across dim 1 to get the expected feature vector\n",
    "# condensed_z = torch.sum(scored_z, dim=1)\n",
    "\n",
    "# condensed_x = condensed_y * condensed_z\n",
    "\n",
    "# print(condensed_y.shape, condensed_z.shape, condensed_x.shape)\n",
    "# print(condensed_y, condensed_z, condensed_x)\n",
    "# '''\n",
    "# x = x * attn_int\n",
    "# x = torch.sum(x, dim=1)\n",
    "# print(x.shape)\n",
    "# '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(2, 2048)\n",
    "\n",
    "torch.cat([x, x], axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yLSGmyr5Iuh0"
   },
   "outputs": [],
   "source": [
    "class CNN_Resnet(nn.Module):\n",
    "    \n",
    "    def __init__(self, fine_tune=False):\n",
    "        \n",
    "        super(CNN_Resnet, self).__init__()\n",
    "        \n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        #resnet.load_state_dict(torch.load('imagenet_models/resnet50-19c8e357.pth'))\n",
    "        \n",
    "        self.attn_row =  Attention(features)\n",
    "        self.attn_col =  Attention(features)\n",
    "        \n",
    "        \n",
    "        # freezing parameters\n",
    "        if not fine_tune:\n",
    "            for param in resnet.parameters():\n",
    "                param.requires_grad = False\n",
    "        else:\n",
    "            \n",
    "            for param in resnet.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "        layers = list(resnet.children())[:-2]\n",
    "        self.features = nn.Sequential(*layers).cuda()\n",
    "        \n",
    "        self.fc = nn.Linear(features * 2, 120)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # Initialize the fc layers.\n",
    "        nn.init.xavier_normal_(self.fc.weight.data)\n",
    "        \n",
    "        if self.fc.bias is not None:\n",
    "            torch.nn.init.constant_(self.fc.bias.data, val=0)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        ## X: bs, 3, 256, 256\n",
    "        ## N = bs\n",
    "        N = x.size()[0]\n",
    "        \n",
    "        ## x : bs, depth_size, fmap_size, fmap_size\n",
    "        x = self.features(x)\n",
    "\n",
    "        # bs, \n",
    "        x = x.view(N, features, fmap_size ** 2)\n",
    "        \n",
    "        \n",
    "        x = torch.bmm(x, torch.transpose(x, 1, 2))/ (fmap_size ** 2) \n",
    "        x = torch.sqrt(x + 1e-5)\n",
    "        \n",
    "\n",
    "        y = self.attn_row(x)\n",
    "        \n",
    "        z = self.attn_col(x.permute(0, 2, 1))\n",
    "        \n",
    "        \n",
    "        scored_y = x * y\n",
    "        \n",
    "        # now, sum across dim 1 to get the expected feature vector\n",
    "        condensed_y = torch.sum(scored_y, dim=1)\n",
    "\n",
    "        scored_z = x * z\n",
    "        \n",
    "        # now, sum across dim 1 to get the expected feature vector\n",
    "        condensed_z = torch.sum(scored_z, dim=1)\n",
    "        \n",
    "        condensed_x = torch.cat([condensed_y, condensed_z], axis=1)\n",
    "        \n",
    "        x = self.dropout(condensed_x)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## additive noise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = CNN_Resnet().cuda()\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "\n",
    "# lr_finder = LRFinder(model, optimizer, criterion, device=\"cuda\")\n",
    "# lr_finder.range_test(train_loader, end_lr=1, num_iter=100)\n",
    "# lr_finder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN_Resnet().cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.class_to_idx = train_dataset.class_to_idx\n",
    "model.idx_to_class = {\n",
    "    idx: class_\n",
    "    for class_, idx in model.class_to_idx.items()\n",
    "}\n",
    "\n",
    "list(model.idx_to_class.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jEmypS6AfKFs"
   },
   "outputs": [],
   "source": [
    "def train(model, \n",
    "          criterion, \n",
    "          optimizer, \n",
    "          train_loader,\n",
    "          val_loader, \n",
    "          save_location, \n",
    "          early_stop=3, \n",
    "          n_epochs=20, \n",
    "          print_every=1):\n",
    "\n",
    "    #Initializing some variables\n",
    "    valid_acc_max = 0\n",
    "    stop_count = 0\n",
    "    history = []\n",
    "    model.epochs = 0\n",
    "\n",
    "    #Loop starts here\n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        train_loss = 0\n",
    "        valid_loss = 0\n",
    "\n",
    "        train_acc = 0\n",
    "        valid_acc = 0\n",
    "\n",
    "        model.train()\n",
    "        \n",
    "        ### batch control\n",
    "        ii = 0\n",
    "        \n",
    "        for data, label in train_loader:\n",
    "            \n",
    "            ii += 1\n",
    "            \n",
    "            data, label = data.cuda(), label.cuda()\n",
    "            output = model(data)\n",
    "            \n",
    "            loss = criterion(output, label)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Track train loss by multiplying average loss by number of examples in batch\n",
    "            train_loss += loss.item() * data.size(0)\n",
    "            \n",
    "            \n",
    "            # Calculate accuracy by finding max log probability\n",
    "            # first output gives the max value in the row(not what we want), second output gives index of the highest val\n",
    "            _, pred = torch.max(output, dim=1)\n",
    "            \n",
    "            # using the index of the predicted outcome above, torch.eq() will check prediction index against label index to see if prediction is correct(returns 1 if correct, 0 if not)\n",
    "            correct_tensor = pred.eq(label.data.view_as(pred))\n",
    "            \n",
    "            #tensor must be float to calc average\n",
    "            accuracy = torch.mean(correct_tensor.type(torch.FloatTensor))\n",
    "            train_acc += accuracy.item() * data.size(0)\n",
    "            \n",
    "            if ii%10 == 0:\n",
    "                print(f'Epoch: {epoch}\\t{100 * (ii + 1) / len(train_loader):.2f}% complete.')\n",
    "        \n",
    "        model.epochs += 1\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            model.eval()\n",
    "            \n",
    "            for data, label in val_loader:\n",
    "                \n",
    "                data, label = data.cuda(), label.cuda()\n",
    "                output = model(data)\n",
    "                loss = criterion(output, label)\n",
    "                valid_loss += loss.item() * data.size(0)\n",
    "                \n",
    "                _, pred = torch.max(output, dim=1)\n",
    "                correct_tensor = pred.eq(label.data.view_as(pred))\n",
    "                accuracy = torch.mean(correct_tensor.type(torch.FloatTensor))\n",
    "                valid_acc += accuracy.item() * data.size(0)\n",
    "            \n",
    "            train_loss = train_loss / len(train_loader.dataset)\n",
    "            valid_loss = valid_loss / len(val_loader.dataset)\n",
    "\n",
    "            train_acc = train_acc / len(train_loader.dataset)\n",
    "            valid_acc = valid_acc / len(val_loader.dataset)\n",
    "\n",
    "            history.append([train_loss, valid_loss, train_acc, valid_acc])\n",
    "\n",
    "            if (epoch + 1) % print_every == 0:\n",
    "                \n",
    "                print(f'\\nEpoch: {epoch} \\tTraining Loss: {train_loss:.4f} \\tValidation Loss: {valid_loss:.4f}')\n",
    "                print(f'\\t\\tTraining Accuracy: {100 * train_acc:.2f}%\\t Validation Accuracy: {100 * valid_acc:.2f}%')\n",
    "\n",
    "            if valid_acc > valid_acc_max:\n",
    "                \n",
    "                torch.save({\n",
    "                    'state_dict': model.state_dict()\n",
    "                    #'idx_to_class': model.idx_to_class\n",
    "                }, save_location)\n",
    "                \n",
    "                stop_count = 0\n",
    "                valid_acc_max = valid_acc\n",
    "                best_epoch = epoch\n",
    "                \n",
    "            else:\n",
    "                \n",
    "                stop_count += 1\n",
    "                \n",
    "                # Below is the case where we handle the early stop case\n",
    "                if stop_count >= early_stop:\n",
    "                    \n",
    "                    print(f'\\nEarly Stopping Total epochs: {epoch}. Best epoch: {best_epoch} with best val acc: {100 * valid_acc_max:.2f}%')\n",
    "                    model.load_state_dict(torch.load(save_location)['state_dict'])\n",
    "                    model.optimizer = optimizer\n",
    "                    history = pd.DataFrame(history, columns=['train_loss', 'valid_loss', 'train_acc','valid_acc'])\n",
    "                    return model, history\n",
    "    \n",
    "    model.optimizer = optimizer\n",
    "    \n",
    "    history = pd.DataFrame(history, columns=['train_loss', 'valid_loss', 'train_acc', 'valid_acc'])\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summary(model, input_size=(3, image_size, image_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, history = train(\n",
    "    model,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    save_location='/kaggle/working/dog_bcnn_resnet50_row_col_agg_conc.pt',\n",
    "    early_stop=3,\n",
    "    n_epochs=10,\n",
    "    print_every=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Resnet(nn.Module):\n",
    "    \n",
    "    def __init__(self, fine_tune=False):\n",
    "        \n",
    "        super(CNN_Resnet, self).__init__()\n",
    "        \n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        #resnet.load_state_dict(torch.load('imagenet_models/resnet50-19c8e357.pth'))\n",
    "        \n",
    "        self.attn_row =  Attention(features)\n",
    "        self.attn_col =  Attention(features)\n",
    "        \n",
    "        \n",
    "        # freezing parameters\n",
    "        if not fine_tune:\n",
    "            for param in resnet.parameters():\n",
    "                param.requires_grad = False\n",
    "        else:\n",
    "            \n",
    "            for param in resnet.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "        layers = list(resnet.children())[:-2]\n",
    "        self.features = nn.Sequential(*layers).cuda()\n",
    "        \n",
    "        self.fc = nn.Linear(features * 2, 120)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # Initialize the fc layers.\n",
    "        nn.init.xavier_normal_(self.fc.weight.data)\n",
    "        \n",
    "        if self.fc.bias is not None:\n",
    "            torch.nn.init.constant_(self.fc.bias.data, val=0)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        ## X: bs, 3, 256, 256\n",
    "        ## N = bs\n",
    "        N = x.size()[0]\n",
    "        \n",
    "        ## x : bs, depth_size, fmap_size, fmap_size\n",
    "        x = self.features(x)\n",
    "\n",
    "        # bs, \n",
    "        x = x.view(N, features, fmap_size ** 2)\n",
    "        \n",
    "        \n",
    "        x = torch.bmm(x, torch.transpose(x, 1, 2))/ (fmap_size ** 2) \n",
    "        x = torch.sqrt(x + 1e-5)\n",
    "        \n",
    "\n",
    "        y = self.attn_row(x)\n",
    "        \n",
    "        z = self.attn_col(x.permute(0, 2, 1))\n",
    "        \n",
    "        \n",
    "        scored_y = x * y\n",
    "        \n",
    "        # now, sum across dim 1 to get the expected feature vector\n",
    "        condensed_y = torch.sum(scored_y, dim=1)\n",
    "\n",
    "        scored_z = x * z\n",
    "        \n",
    "        # now, sum across dim 1 to get the expected feature vector\n",
    "        condensed_z = torch.sum(scored_z, dim=1)\n",
    "        \n",
    "        condensed_x = torch.cat([condensed_y, condensed_z], axis=1)\n",
    "        \n",
    "        x = self.dropout(condensed_x)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return condensed_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN_Resnet(fine_tune=False).cuda()\n",
    "model.load_state_dict(torch.load('/kaggle/working/dog_bcnn_resnet50_row_col_agg_conc.pt')['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_deep_features(img_path):\n",
    "    \n",
    "    img = Image.open(img_path)\n",
    "    arr_prep = image_transforms[\"val\"](img)\n",
    "    arr_prep = arr_prep.unsqueeze(axis=0).cuda()\n",
    "    \n",
    "    \n",
    "    arr_prep= arr_prep.repeat(2, 1, 1, 1)\n",
    "    \n",
    "    feat = model(arr_prep)\n",
    "    print(feat.shape)\n",
    "    return feat[0].cpu().detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_data/n02094258-Norwich_terrier/n02094258_103.jpg\n",
    "\n",
    "cropped_data/n02086240-Shih-Tzu/n02086240_1078.jpg\n",
    "\n",
    "cropped_data/n02093991-Irish_terrier/n02093991_2437.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine as cs\n",
    "from scipy.spatial.distance import euclidean as eu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eu(x2,x3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eu(x1,x3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.__version__\n",
    "torchvision.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resnet = models.resnet50(pretrained=True)\n",
    "\n",
    "\n",
    "# #resnet.load_state_dict(torch.load('/kaggle/input/pretrained-pytorch-models/resnet50-19c8e357.pth'))\n",
    "# layers = list(models.resnet50().children())[:-1]\n",
    "# resnet = nn.Sequential(*layers).cuda()\n",
    "# resnet.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = models.resnet50(pretrained=True)\n",
    "\n",
    "\n",
    "#resnet.load_state_dict(torch.load('/kaggle/input/pretrained-pytorch-models/resnet50-19c8e357.pth'))\n",
    "layers = list(models.resnet50().children())[:-1]\n",
    "resnet = nn.Sequential(*layers)\n",
    "resnet.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def get_deep_features(img_path):\n",
    "    \n",
    "#     resnet.eval()\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "        \n",
    "#         img = Image.open(img_path)\n",
    "#         arr_prep = image_transforms[\"val\"](img)\n",
    "#         arr_prep = arr_prep.unsqueeze(axis=0).cuda()\n",
    "#         feat = resnet(arr_prep)\n",
    "#         return feat.view(1, -1).cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_deep_features(img_path):\n",
    "    \n",
    "    resnet.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        img = Image.open(img_path)\n",
    "        arr_prep = image_transforms[\"val\"](img)\n",
    "        arr_prep = arr_prep.unsqueeze(axis=0)\n",
    "        feat = resnet(arr_prep)\n",
    "        return feat.view(1, -1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "x1 = get_deep_features('/kaggle/working/cropped_data/n02094258-Norwich_terrier/n02094258_103.jpg')\n",
    "x2 = get_deep_features('/kaggle/working/cropped_data/n02109961-Eskimo_dog/n02109961_12719.jpg')\n",
    "x3 = get_deep_features('/kaggle/working/cropped_data/n02109961-Eskimo_dog/n02109961_1017.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 - cs(x1,x3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 - cs(x2,x3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BCNN_for_CUS-200.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "clova_tr",
   "language": "python",
   "name": "clova_tr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
