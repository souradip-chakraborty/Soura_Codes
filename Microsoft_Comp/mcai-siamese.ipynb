{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import Input, Lambda,subtract,GlobalMaxPooling2D,Dense,GlobalAveragePooling2D, concatenate, Activation\n",
    "from keras.applications.mobilenet import MobileNet as Net\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential, model_from_json\n",
    "from keras.layers import Dense, LSTM, Dropout, Bidirectional, merge\n",
    "from keras.layers.convolutional import Convolution1D, MaxPooling1D\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from IPython.display import SVG\n",
    "#from keras.utils.visualize_util import model_to_dot\n",
    "from nltk.corpus import stopwords\n",
    "import wikipedia\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from multiprocessing import Pool\n",
    "\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.mobilenet import preprocess_input\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, merge\n",
    "from keras.optimizers import Adam\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from keras.models import Sequential\n",
    "\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "\n",
    "ALPHA = 0.3  # Triplet Loss Parameter\n",
    "\n",
    "from keras.layers import Input,Lambda,subtract,GlobalMaxPooling2D,Dense,GlobalAveragePooling2D,concatenate,Activation\n",
    "from keras.applications.xception import Xception as Net\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.xception import preprocess_input\n",
    "from keras.models import Model\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Embedding, Flatten, Input, merge\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "#import data\n",
    "#import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Data\n",
    "df_train = pd.read_csv('/Users/s0c02nj/Desktop/Personal Edu doc/Competitions /Microsoft/data/data.tsv', \n",
    "                       delimiter=\"\\t\",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Data\n",
    "df_test = pd.read_csv('/Users/s0c02nj/Desktop/Personal Edu doc/Competitions /Microsoft/data/eval1_unlabelled.tsv', \n",
    "                      delimiter=\"\\t\",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Column names Train\n",
    "df_train.columns = [\"id\", \"question\", \"answer\", \"is_ans\", \"seq_no\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Column names Test\n",
    "df_test.columns = [\"id\", \"question\", \"answer\", \"seq_no\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_train = list(map(str, df_train['question'].tolist()))\n",
    "q2_train = list(map(str, df_train['answer'].tolist()))\n",
    "is_ans = df_train['is_ans'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_train = q1_train[:100000]\n",
    "q2_train = q2_train[:100000]\n",
    "is_ans = is_ans[:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#length of the seq\n",
    "MAX_SEQUENCE_LENGTH = 200\n",
    "#For tokenizer how many words needed\n",
    "MAX_NB_WORDS = 100000\n",
    "#Dimension of the embed matrix\n",
    "EMBEDDING_DIM = 50\n",
    "#validation split for model\n",
    "VALIDATION_SPLIT = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/s0c02nj/anaconda2/lib/python2.7/site-packages/keras/preprocessing/text.py:145: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
     ]
    }
   ],
   "source": [
    "#Tokenize the documnent into tokens for each word in doc\n",
    "tokenizer = Tokenizer(nb_words=MAX_NB_WORDS, lower=True)\n",
    "tokenizer.fit_on_texts(q1_train + q2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path= '/Users/s0c02nj/Downloads/glove.6B/glove.6B.50d.txt'\n",
    "f=open(folder_path)\n",
    "doc=f.readlines()\n",
    "#****VIP\n",
    "#**WORD TO VEC DICTIONARY\n",
    "#Forming a dictionary-word2vec\n",
    "word2vec={}\n",
    "key=[]\n",
    "#looping though the doc.in the doc the entire thing is saved and is separated by a space bar.\n",
    "for line in doc:\n",
    "    #parts contains every word separately for doc1\n",
    "    parts=line.split(' ')\n",
    "    #part[0] contains the word\n",
    "    word=parts[0]\n",
    "    key.append(word)\n",
    "    #embed contains the vector\n",
    "    embed=np.array(parts[1:],dtype='float32')\n",
    "    #filling up the dictionary\n",
    "    word2vec[word]=embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "print('Found %s word vectors.' % len(word2vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the tokenized seq for the docs\n",
    "q1_seq = tokenizer.texts_to_sequences(q1_train)\n",
    "q2_seq = tokenizer.texts_to_sequences(q2_train)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "#print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(q1_seq[150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_seq = pad_sequences(q1_seq, maxlen=10, padding='post')\n",
    "q2_seq = pad_sequences(q2_seq, maxlen=70, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(is_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Embedding matrix creation\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index)+1)\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i >= nb_words:\n",
    "        continue\n",
    "    if word in word2vec:\n",
    "        embedding_vector = word2vec[word]\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
