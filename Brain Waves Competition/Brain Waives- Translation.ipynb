{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import Ridge\n",
    "import numpy as np\n",
    "import sklearn.datasets\n",
    "#import sklearn.cross_validation\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn import ensemble\n",
    "import time\n",
    "import pickle\n",
    "import random\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "import keras \n",
    "from keras import backend as K\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from sklearn.preprocessing import OneHotEncoder,StandardScaler\n",
    "import re\n",
    "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding,Bidirectional\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import tensorflow as tf\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation , Convolution1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "import keras \n",
    "from keras import backend as K\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.layers import Merge\n",
    "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding,Bidirectional\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#****VIP\n",
    "#**WORD TO VEC DICTIONARY\n",
    "#Forming a dictionary-word2vec\n",
    "folder_path= '/Users/s0c02nj/Desktop/cc.fr.300.vec'\n",
    "f=open(folder_path)\n",
    "doc=f.readlines()\n",
    "\n",
    "word2vec={}\n",
    "key=[]\n",
    "#looping though the doc.in the doc the entire thing is saved and is separated by a space bar.\n",
    "for line in doc:\n",
    "    #parts contains every word separately for doc1\n",
    "    parts=line.split(' ')\n",
    "    #part[0] contains the word\n",
    "    word=parts[0]\n",
    "    key.append(word.lower())\n",
    "    #embed contains the vector\n",
    "    embed=np.array(parts[1:],dtype='float32')\n",
    "    #filling up the dictionary\n",
    "    word2vec[word]=embed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv('/Users/s0c02nj/Desktop/c3cc8568-0-dataset/train.csv')\n",
    "test=pd.read_csv('/Users/s0c02nj/Desktop/c3cc8568-0-dataset/test.csv')\n",
    "sample_submission=pd.read_csv('/Users/s0c02nj/Desktop/c3cc8568-0-dataset/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_copy=train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Complaint-ID</th>\n",
       "      <th>Date-received</th>\n",
       "      <th>Transaction-Type</th>\n",
       "      <th>Complaint-reason</th>\n",
       "      <th>Company-response</th>\n",
       "      <th>Date-sent-to-company</th>\n",
       "      <th>Complaint-Status</th>\n",
       "      <th>Consumer-disputes</th>\n",
       "      <th>Consumer-complaint-summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tr-1</td>\n",
       "      <td>11/11/2015</td>\n",
       "      <td>Mortgage</td>\n",
       "      <td>Loan servicing, payments, escrow account</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11/11/2015</td>\n",
       "      <td>Closed with explanation</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Seterus, Inc a déposé un faux rapport auprès d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tr-2</td>\n",
       "      <td>7/7/2015</td>\n",
       "      <td>Credit reporting</td>\n",
       "      <td>Incorrect information on credit report</td>\n",
       "      <td>Company chooses not to provide a public response</td>\n",
       "      <td>7/7/2015</td>\n",
       "      <td>Closed with non-monetary relief</td>\n",
       "      <td>No</td>\n",
       "      <td>XX / XX / XXXX La requête en faillite n ° XXXX...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tr-3</td>\n",
       "      <td>5/7/2015</td>\n",
       "      <td>Bank account or service</td>\n",
       "      <td>Using a debit or ATM card</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5/7/2015</td>\n",
       "      <td>Closed with explanation</td>\n",
       "      <td>No</td>\n",
       "      <td>El XXXX / XXXX / 15, estaba preparando el vuel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tr-4</td>\n",
       "      <td>11/12/2016</td>\n",
       "      <td>Debt collection</td>\n",
       "      <td>Cont'd attempts collect debt not owed</td>\n",
       "      <td>Company believes it acted appropriately as aut...</td>\n",
       "      <td>11/12/2016</td>\n",
       "      <td>Closed with explanation</td>\n",
       "      <td>No</td>\n",
       "      <td>The loan was paid in XXXX XXXX. In XXXX, 4 yea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tr-5</td>\n",
       "      <td>9/29/2016</td>\n",
       "      <td>Credit card</td>\n",
       "      <td>Payoff process</td>\n",
       "      <td>Company has responded to the consumer and the ...</td>\n",
       "      <td>9/29/2016</td>\n",
       "      <td>Closed with explanation</td>\n",
       "      <td>No</td>\n",
       "      <td>J'ai obtenu un compte de crédit de soins pour ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Complaint-ID Date-received         Transaction-Type  \\\n",
       "0         Tr-1    11/11/2015                 Mortgage   \n",
       "1         Tr-2      7/7/2015         Credit reporting   \n",
       "2         Tr-3      5/7/2015  Bank account or service   \n",
       "3         Tr-4    11/12/2016          Debt collection   \n",
       "4         Tr-5     9/29/2016              Credit card   \n",
       "\n",
       "                           Complaint-reason  \\\n",
       "0  Loan servicing, payments, escrow account   \n",
       "1    Incorrect information on credit report   \n",
       "2                 Using a debit or ATM card   \n",
       "3     Cont'd attempts collect debt not owed   \n",
       "4                            Payoff process   \n",
       "\n",
       "                                    Company-response Date-sent-to-company  \\\n",
       "0                                                NaN           11/11/2015   \n",
       "1   Company chooses not to provide a public response             7/7/2015   \n",
       "2                                                NaN             5/7/2015   \n",
       "3  Company believes it acted appropriately as aut...           11/12/2016   \n",
       "4  Company has responded to the consumer and the ...            9/29/2016   \n",
       "\n",
       "                  Complaint-Status Consumer-disputes  \\\n",
       "0          Closed with explanation               Yes   \n",
       "1  Closed with non-monetary relief                No   \n",
       "2          Closed with explanation                No   \n",
       "3          Closed with explanation                No   \n",
       "4          Closed with explanation                No   \n",
       "\n",
       "                          Consumer-complaint-summary  \n",
       "0  Seterus, Inc a déposé un faux rapport auprès d...  \n",
       "1  XX / XX / XXXX La requête en faillite n ° XXXX...  \n",
       "2  El XXXX / XXXX / 15, estaba preparando el vuel...  \n",
       "3  The loan was paid in XXXX XXXX. In XXXX, 4 yea...  \n",
       "4  J'ai obtenu un compte de crédit de soins pour ...  "
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Joining Available Columns in Test and Train for Pre-processing\n",
    "join=train_copy.append(test,sort=False)\n",
    "join.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Time \n",
    "join['Date-received'] =  pd.to_datetime(join['Date-received'], format='%m/%d/%Y')\n",
    "join['Date-sent-to-company'] =  pd.to_datetime(join['Date-sent-to-company'], format='%m/%d/%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Date time diff\n",
    "join['diff_time']=join['Date-sent-to-company']-join['Date-received']\n",
    "join['diff_time']=join['diff_time'].apply(lambda x:x.days )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Company Respose--- Mode\n",
    "join['Company-response'].fillna(value=join['Company-response'].value_counts().index[0],inplace =True)\n",
    "#join['Company-response'].fillna(value='No response',inplace =True)\n",
    "\n",
    "\n",
    "#Missing Dispute-- Mode\n",
    "join['Consumer-disputes'].fillna(value=join['Consumer-disputes'].value_counts().index[0],inplace =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standard Scaler\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Time Difference\n",
    "diff_time=np.array(join['diff_time']).reshape(-1,1)\n",
    "\n",
    "#Standadizing time\n",
    "scaler.fit(diff_time)\n",
    "enc_time= scaler.transform(diff_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Label Encoding the Categorical Varaible\n",
    "le1= preprocessing.LabelEncoder()\n",
    "le1.fit(join['Transaction-Type'])\n",
    "enc_trans_type= le1.transform(join['Transaction-Type'])\n",
    "\n",
    "le2= preprocessing.LabelEncoder()\n",
    "le2.fit(join['Complaint-reason'])\n",
    "enc_complain_reason= le2.transform(join['Complaint-reason'])\n",
    "\n",
    "le3= preprocessing.LabelEncoder()\n",
    "le3.fit(join['Company-response'])\n",
    "enc_comp_response= le3.transform(join['Company-response'])\n",
    "\n",
    "le4= preprocessing.LabelEncoder()\n",
    "le4.fit(join['Consumer-disputes'])\n",
    "enc_cons_dispute= le4.transform(join['Consumer-disputes'])\n",
    "\n",
    "le5= preprocessing.LabelEncoder()\n",
    "le5.fit(train['Complaint-Status'])\n",
    "y_cat= le5.transform(train['Complaint-Status'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "y_enc=to_categorical(y_cat,num_classes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-processing\n",
    "train['Consumer-complaint-summary'] = train['Consumer-complaint-summary'].apply(lambda x: re.sub(r'[^a-zA-Z]', ' ', x))\n",
    "test['Consumer-complaint-summary'] = test['Consumer-complaint-summary'].apply(lambda x: re.sub(r'[^a-zA-Z]', ' ', x))\n",
    "\n",
    "train['Consumer-complaint-summary'] = train['Consumer-complaint-summary'].apply(lambda x: str(x))\n",
    "test['Consumer-complaint-summary'] = test['Consumer-complaint-summary'].apply(lambda x: str(x))\n",
    "\n",
    "train['Consumer-complaint-summary']=train['Consumer-complaint-summary'].str.replace('XX','')\n",
    "test['Consumer-complaint-summary']=test['Consumer-complaint-summary'].str.replace('XX','')\n",
    "\n",
    "train['Consumer-complaint-summary']=train['Consumer-complaint-summary'].apply(lambda x:re.sub('\\s+', ' ', x).strip())\n",
    "test['Consumer-complaint-summary']=test['Consumer-complaint-summary'].apply(lambda x:re.sub('\\s+', ' ', x).strip())\n",
    "\n",
    "train['Consumer-complaint-summary']=train['Consumer-complaint-summary'].apply(lambda x:x.lower())\n",
    "test['Consumer-complaint-summary']=test['Consumer-complaint-summary'].apply(lambda x:x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 80000\n",
    "tokenizer = Tokenizer(lower = True, filters='', num_words=max_features)\n",
    "full_text = list(join['Consumer-complaint-summary'].values)\n",
    "tokenizer.fit_on_texts(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizing the documents---- convert to strings\n",
    "train_tokenized = tokenizer.texts_to_sequences(train['Consumer-complaint-summary'].fillna('missing'))\n",
    "test_tokenized=   tokenizer.texts_to_sequences(test ['Consumer-complaint-summary'].fillna('missing'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Padding the same\n",
    "max_len = 500\n",
    "X_train = pad_sequences(train_tokenized, maxlen = max_len,padding='post')\n",
    "X_test = pad_sequences(test_tokenized, maxlen = max_len,padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 60526 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "#word_index is dictionary of the words and the sequence\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null word embeddings: 1\n"
     ]
    }
   ],
   "source": [
    "#Embedding matrix creation\n",
    "nb_words = min(max_features, len(word_index)+1)\n",
    "embedding_matrix = np.zeros((nb_words, 300))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    #print i\n",
    "    if i >= nb_words:\n",
    "        continue\n",
    "    if word in word2vec:\n",
    "        embedding_vector = word2vec[word]\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transaction\n",
    "enc_trans_type_train=enc_trans_type[0:43266]\n",
    "enc_trans_type_test=enc_trans_type[43266:]\n",
    "\n",
    "#Complaint reason\n",
    "enc_complain_reason_train=enc_complain_reason[0:43266]\n",
    "enc_complain_reason_test=enc_complain_reason[43266:]\n",
    "\n",
    "enc_comp_response_train=enc_comp_response[0:43266]\n",
    "enc_comp_response_test= enc_comp_response[43266:]\n",
    "\n",
    "enc_cons_dispute_train=enc_cons_dispute[0:43266]\n",
    "enc_cons_dispute_test= enc_cons_dispute[43266:]\n",
    "\n",
    "enc_time_train=enc_time[0:43266]\n",
    "enc_time_test=enc_time[43266:]\n",
    "\n",
    "tf_idf_arr_train=tf_idf_arr[0:43266]\n",
    "tf_idf_arr_test=tf_idf_arr[43266:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_lstm():\n",
    "    \n",
    "    #Defining the input-----> Transaction\n",
    "    inputs1 = Input(shape=(1,))\n",
    "    layer_transaction = Embedding(18 , 7 ,input_length=1)(inputs1)\n",
    "    layer_transaction = Flatten()(layer_transaction)\n",
    "    \n",
    "    #Defining the input-----> Complaint-reason\n",
    "    inputs2 = Input(shape=(1,))\n",
    "    layer_complain_reason = Embedding(152, 60, input_length=1)(inputs2)\n",
    "    layer_complain_reason = Flatten()(layer_complain_reason)\n",
    "    \n",
    "    #Defining the input-----> Complaint-reason\n",
    "    inputs3 = Input(shape=(1,))\n",
    "    layer_company_response = Embedding(11 ,5,input_length=1)(inputs3)\n",
    "    layer_company_response = Flatten()(layer_company_response)\n",
    "    \n",
    "    #Defining the input-----> Consumer-disputes\n",
    "    inputs4 = Input(shape=(1,))\n",
    "    layer_consumer_disputes = Embedding(2 ,2,input_length=1)(inputs4)\n",
    "    layer_consumer_disputes = Flatten()(layer_consumer_disputes)\n",
    "    \n",
    "    #Defining the input ----> Time\n",
    "    inputs5 = Input(shape=(1,))\n",
    "    \n",
    "    #Defining the input-----> Consumer-complaint-summary\n",
    "    inputs6 = Input(shape=(max_len,))\n",
    "    layer_complaint =  Embedding(nb_words, 300, input_length=max_len,weights = [embedding_matrix],trainable=False)(inputs6)\n",
    "    \n",
    "    #LSTM\n",
    "    layer_lstm = LSTM(128, return_sequences=True)(layer_complaint)\n",
    "    \n",
    "    #Convolution layer\n",
    "    conv_layer = Convolution1D(128, 2, activation='relu')(layer_lstm)\n",
    "    conv_layer = MaxPooling1D(499)(conv_layer)\n",
    "    conv_layer = Flatten()(conv_layer)\n",
    "    \n",
    "    #Merge\n",
    "    layer_merge= concatenate([layer_transaction, \n",
    "                              layer_complain_reason, \n",
    "                              layer_company_response, \n",
    "                              layer_consumer_disputes,\n",
    "                              inputs5, \n",
    "                              conv_layer], axis=1)\n",
    "    \n",
    "    #Dense Layers\n",
    "    layer_dense = Dense(64, activation='relu')(layer_merge)\n",
    "    layer_dense = Dropout(0.3)(layer_merge)\n",
    "    \n",
    "    #Output Layer\n",
    "    probabilities = Dense(5,activation='softmax')(layer_dense)\n",
    "\n",
    "    model = Model(inputs=[inputs1, inputs2, inputs3, inputs4, inputs5, inputs6], outputs=probabilities)\n",
    "    return model\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_200 (InputLayer)          (None, 500)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_144 (Embedding)       (None, 500, 300)     18158100    input_200[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_10 (LSTM)                  (None, 500, 128)     219648      embedding_144[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "input_195 (InputLayer)          (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_196 (InputLayer)          (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_197 (InputLayer)          (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_198 (InputLayer)          (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 499, 128)     32896       lstm_10[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_140 (Embedding)       (None, 1, 7)         126         input_195[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "embedding_141 (Embedding)       (None, 1, 60)        9120        input_196[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "embedding_142 (Embedding)       (None, 1, 5)         55          input_197[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "embedding_143 (Embedding)       (None, 1, 2)         4           input_198[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 1, 128)       0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_134 (Flatten)           (None, 7)            0           embedding_140[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_135 (Flatten)           (None, 60)           0           embedding_141[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_136 (Flatten)           (None, 5)            0           embedding_142[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_137 (Flatten)           (None, 2)            0           embedding_143[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "input_199 (InputLayer)          (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_138 (Flatten)           (None, 128)          0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_23 (Concatenate)    (None, 203)          0           flatten_134[0][0]                \n",
      "                                                                 flatten_135[0][0]                \n",
      "                                                                 flatten_136[0][0]                \n",
      "                                                                 flatten_137[0][0]                \n",
      "                                                                 input_199[0][0]                  \n",
      "                                                                 flatten_138[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_27 (Dropout)            (None, 203)          0           concatenate_23[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_55 (Dense)                (None, 5)            1020        dropout_27[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 18,420,969\n",
      "Trainable params: 262,869\n",
      "Non-trainable params: 18,158,100\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=model_lstm()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = \"categorical_crossentropy\", optimizer = Adam(lr=0.001), metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 38939 samples, validate on 4327 samples\n",
      "Epoch 1/1\n",
      "33792/38939 [=========================>....] - ETA: 1:05 - loss: 0.7176 - acc: 0.7913"
     ]
    }
   ],
   "source": [
    "history = model.fit([enc_trans_type_train,\n",
    "                     enc_complain_reason_train,\n",
    "                     enc_comp_response_train,\n",
    "                     enc_cons_dispute_train,\n",
    "                     enc_time_train,\n",
    "                     X_train] ,y_enc, batch_size = 1024, epochs = 1, validation_split=0.1, \n",
    "                     verbose = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
