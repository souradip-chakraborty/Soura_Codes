{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "from deepwalk import graph\n",
    "from gensim.models import Word2Vec\n",
    "import os\n",
    "import psutil\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "from multiprocessing import cpu_count\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.optimizers import RMSprop ,Adam\n",
    "from keras import backend as K\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation,Subtract,Multiply\n",
    "from keras.layers import Activation, Dense, Dropout, Input, Embedding,concatenate\n",
    "from keras.layers.embeddings import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install deepwalk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('/Users/s0c02nj/Desktop/Personal Edu doc/Competitions /Hike/train/train.csv')\n",
    "train_feat = pd.read_csv('/Users/s0c02nj/Desktop/Personal Edu doc/Competitions /Hike/train/user_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('/Users/s0c02nj/Desktop/Personal Edu doc/Competitions /Hike/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_data['node1_id']\n",
    "list_test_nodes = list(np.unique(list(test_data['node1_id'].unique()) + list(test_data['node2_id'].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subsetting the Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_ones = train_data[train_data['is_chat']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_zeros = train_data[train_data['is_chat']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_sub_ones = train_data_ones[(train_data_ones['node1_id'].isin(list_test_nodes)) | \n",
    "                            (train_data_ones['node2_id'].isin(list_test_nodes))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_sub_ones.index = range(0,len(train_data_sub_ones))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_sub_zeros = train_data_zeros[(train_data_zeros['node1_id'].isin(list_test_nodes)) | \n",
    "                            (train_data_zeros['node2_id'].isin(list_test_nodes))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_sub_zeros.index = range(0,len(train_data_sub_zeros))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_sub_ones = train_data_sub_ones[0:72244]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_sub_zeros = train_data_sub_zeros[0:72244]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.concat([train_data_sub_ones,train_data_sub_zeros],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_data['is_chat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_cat = to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Random Walk through Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ones = train_data[train_data['is_chat']==1]\n",
    "x_ones.index = range(0,len(x_ones))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = list(np.unique(list(train_data['node1_id'].unique()) + list(train_data['node2_id'].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph()\n",
    "G.add_nodes_from(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ls1=[]\n",
    "for i in range(0,len(x_ones)) :\n",
    "    x_ls1.append((x_ones['node1_id'].iloc[i],x_ones['node2_id'].iloc[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add edges from txt \n",
    "G.add_edges_from(x_ls1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(G.neighbors(7791327))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val = list(dict_nbr.values())\n",
    "# z=[]\n",
    "# for i in tqdm(val):\n",
    "#     z.append(len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mapping for word2id\n",
    "node2idx = {w: i  for i, w in enumerate(a)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(node2idx)\n",
    "#len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dict_nbr = {}\n",
    "for i in tqdm(range(0,len(a))):\n",
    "    node_id = a[i]\n",
    "    #node_id_up = node2idx[a[i]]\n",
    "    ls_nbr = 2*[node_id]+list(G.neighbors(node_id))\n",
    "    \n",
    "    if len(ls_nbr)<7:\n",
    "        len_rem = 7 - len(ls_nbr)\n",
    "        ls_nbr1 = len_rem*[node_id] + ls_nbr\n",
    "    \n",
    "    elif len(ls_nbr)>=7:\n",
    "        ls_nbr1 = ls_nbr[0:7]\n",
    "    \n",
    "    dict_nbr[node_id]=ls_nbr1\n",
    "    ls_nbr1=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Embedding matrix creation\n",
    "nb_nodes = len(node2idx)\n",
    "embedding_matrix = np.zeros((nb_nodes, 13))\n",
    "\n",
    "for node, i in tqdm(node2idx.items()):\n",
    "    #print i\n",
    "    if i >= nb_nodes:\n",
    "        continue\n",
    "    \n",
    "    embedding_vector = list(train_feat[train_feat['node_id']==node].values[0][1:])\n",
    "    embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "print('Null node embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_nbr_upd={}\n",
    "temp=[]\n",
    "for node, ls in tqdm(dict_nbr.items()):\n",
    "    #print i\n",
    "    node_id = node\n",
    "    for nb_nodes in ls:\n",
    "        nb1 = node2idx[nb_nodes]\n",
    "        temp.append(nb1)\n",
    "    dict_nbr_upd[node_id] = temp\n",
    "    temp =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dict_nbr_upd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_feat1=[]\n",
    "x_feat2=[]\n",
    "\n",
    "\n",
    "for i in tqdm(range(0,len(train_data))):\n",
    "    \n",
    "    node1 = train_data['node1_id'].iloc[i]\n",
    "    node2 = train_data['node2_id'].iloc[i]\n",
    "    \n",
    "    x_feat1.append(dict_nbr_upd[node1])\n",
    "    x_feat2.append(dict_nbr_upd[node2])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layers = [[128, 6, 2],\n",
    "               [128, 5, 2],\n",
    "               [128, 4, 2],\n",
    "               [128, 3, 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_cnn_1D():\n",
    "    \n",
    "    #Input\n",
    "    left_input =  Input(shape=(max_len,))\n",
    "    right_input = Input(shape=(max_len,))\n",
    "    \n",
    "    #Embedding Matrix\n",
    "    layer =  Embedding(136526, 13, weights=[embedding_matrix], input_length=max_len,trainable=True)\n",
    "                                         \n",
    "    \n",
    "    # Embedded version of the inputs\n",
    "    encoded_left =  layer(left_input)\n",
    "    encoded_right = layer(right_input)\n",
    "    \n",
    "    #Convolution with Filter size 2\n",
    "    #x3 = Conv1D(128, 2, activation='relu')(encoded_left)\n",
    "    #x3 = MaxPooling1D(pool_size = 2)(x3)\n",
    "    #x3 = Flatten()(x3)\n",
    "    \n",
    "    #Embedding for left\n",
    "    for filter_num, filter_size, pooling_size in conv_layers:\n",
    "        x1 = Conv1D(filter_num, filter_size)(encoded_left)\n",
    "        x1 = MaxPooling1D(pool_size=pooling_size)(x1) \n",
    "        x1 = Activation('relu')(x1)\n",
    "    x1 = Flatten()(x1)  \n",
    "\n",
    "    \n",
    "    #Embedding for right\n",
    "    for filter_num, filter_size, pooling_size in conv_layers:\n",
    "        x2 = Conv1D(filter_num, filter_size)(encoded_right)\n",
    "        x2 = Activation('relu')(x2)\n",
    "        x2 = MaxPooling1D(pool_size=pooling_size)(x2)  \n",
    "    x2 = Flatten()(x2)  \n",
    "    \n",
    "    #Subtraction\n",
    "    subtracted = Subtract()([x1, x2])\n",
    "    \n",
    "    #multiply\n",
    "    multiply =  Multiply()([x1, x2])\n",
    "    \n",
    "    #Concatenating\n",
    "    merged = concatenate([x1,x2,subtracted,multiply],axis=1)\n",
    "    \n",
    "    #Dense Layer\n",
    "    layer_dense = Dense(128, activation='relu')(merged)\n",
    "    layer_dense = Dropout(0.2)(layer_dense)\n",
    "    \n",
    "    #Output Layer\n",
    "    loss_total = Dense(2,activation='softmax')(layer_dense)\n",
    "\n",
    "    model = Model(inputs=[left_input,right_input],outputs=loss_total)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_cnn_1D()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auc(y_true, y_pred):\n",
    "    auc = tf.metrics.auc(y_true, y_pred)[1]\n",
    "    K.get_session().run(tf.local_variables_initializer())\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = \"binary_crossentropy\", optimizer = Adam(lr=0.001), metrics = [auc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deep Learning Model\n",
    "history = model.fit([x_feat1,\n",
    "                     x_feat2] ,\n",
    "                     y_cat, batch_size = 1024, epochs = 10, validation_split=0.1,\n",
    "                     class_weight = {0:2 ,1:40}, \n",
    "                     verbose = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#val[1116514]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
