{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import re\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/s0c02nj/Downloads/000000000000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing the Nulls\n",
    "df = df[df['abstract'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(text):\n",
    "    text= text.lower()\n",
    "    text= re.sub(r'[^a-z]',' ',text)\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['abstract'] = df['abstract'].apply(lambda x: text_preprocessing(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub = df[0:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sub.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Wordvectors from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = [row.split() for row in df_sub['abstract']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ size: The number of dimensions of the embeddings and the default is 100.\n",
    "+ window: The maximum distance between a target word and words around the target word. The default window is 5.\n",
    "+ min_count: The minimum count of words to consider when training the model; words with occurrence less than this count will be ignored. The default for min_count is 5.\n",
    "+ workers: The number of partitions during training and the default workers is 3.\n",
    "+ sg: The training algorithm, either CBOW(0) or skip gram(1). The default training algorithm is CBOW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from gensim.models.callbacks import CallbackAny2Vec\n",
    "\n",
    "# class MonitorCallback(CallbackAny2Vec):\n",
    "#     def __init__(self, test_words):\n",
    "#         self._test_words = test_words\n",
    "\n",
    "#     def on_epoch_end(self, model):\n",
    "#         print(\"Model loss:\", model.get_latest_training_loss())  # print loss\n",
    "#         for word in self._test_words:  # show wv logic changes\n",
    "#             print(model.wv.most_similar(word))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# monitor = MonitorCallback([\"word\", \"I\", \"less\"])  # monitor with demo words\n",
    "model = Word2Vec(sent, min_count=1,\n",
    "                 size= 100,workers=5, \n",
    "                 window = 5, sg = 1,\n",
    "                 iter= 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.similarity('centrolobular', 'biology')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/s0c02nj/anaconda2/envs/graph/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('concocted', 0.716713547706604),\n",
       " ('ethnopharmacology', 0.6646193265914917),\n",
       " ('formalization', 0.648200511932373),\n",
       " ('biofactories', 0.6468801498413086),\n",
       " ('phospholipidomics', 0.6439762115478516),\n",
       " ('adsorbability', 0.6369298696517944),\n",
       " ('hyperthermophiles', 0.6348873376846313),\n",
       " ('expounded', 0.6347710490226746),\n",
       " ('immunophysiology', 0.6271086931228638),\n",
       " ('ecophysiology', 0.624403715133667)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('corona')[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/s0c02nj/anaconda2/envs/graph/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('breast', 0.8504806756973267),\n",
       " ('prostate', 0.8031083345413208),\n",
       " ('colorectal', 0.7847310304641724),\n",
       " ('cancers', 0.7723956108093262),\n",
       " ('metastatic', 0.7269339561462402),\n",
       " ('ovarian', 0.7181655168533325),\n",
       " ('nonpolyposis', 0.7124499082565308),\n",
       " ('colon', 0.7021201848983765),\n",
       " ('infiltrate', 0.701188325881958),\n",
       " ('metastasis', 0.6927981376647949)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('cancer')[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/s0c02nj/anaconda2/envs/graph/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('anticoagulated', 0.6903988122940063),\n",
       " ('peripheral', 0.6573323011398315),\n",
       " ('pentothal', 0.6478234529495239),\n",
       " ('ompb', 0.6457477807998657),\n",
       " ('cumulatively', 0.6424598693847656),\n",
       " ('circulate', 0.6406136751174927),\n",
       " ('transplantations', 0.6382408142089844),\n",
       " ('bioelectric', 0.6346719264984131),\n",
       " ('abo', 0.6340780258178711),\n",
       " ('cardiologist', 0.6307409405708313)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('blood')[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/s0c02nj/anaconda2/envs/graph/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('mumps', 0.6371492743492126),\n",
       " ('deadliest', 0.6323738098144531),\n",
       " ('wn', 0.6193090677261353),\n",
       " ('illness', 0.6190268993377686),\n",
       " ('denv', 0.5879278182983398),\n",
       " ('yfv', 0.5780838131904602),\n",
       " ('dhf', 0.5715970993041992),\n",
       " ('bloating', 0.5686979293823242),\n",
       " ('macrovascular', 0.5671080350875854),\n",
       " ('pneumonia', 0.5640193223953247)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('fever')[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/s0c02nj/anaconda2/envs/graph/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('immunodiagnostic', 0.6712440252304077),\n",
       " ('zika', 0.631335973739624),\n",
       " ('chikungunya', 0.5942981243133545),\n",
       " ('leptospirosis', 0.5911474227905273),\n",
       " ('pruritus', 0.5886905789375305),\n",
       " ('denv', 0.5832569003105164),\n",
       " ('evasion', 0.5579187870025635),\n",
       " ('cpsv', 0.5561636686325073),\n",
       " ('allergy', 0.5519628524780273),\n",
       " ('teno', 0.5500648021697998)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('dengue')[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class WordVocab(object):\n",
    "\n",
    "#     def __init__(self, mincount=1):\n",
    "#         self.counter = Counter()\n",
    "#         self.n_documents = 0\n",
    "#         self._counters = {}\n",
    "#         self._n_documents = defaultdict(int)\n",
    "#         self.mincount = mincount\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.token2id)\n",
    "\n",
    "#     def add_documents(self, documents, name):\n",
    "#         self._counters[name] = Counter()\n",
    "#         for document in documents:\n",
    "#             bow = dict.fromkeys(document, 1)\n",
    "#             self._counters[name].update(bow)\n",
    "#             self.counter.update(bow)\n",
    "#             self.n_documents += 1\n",
    "#             self._n_documents[name] += 1\n",
    "\n",
    "#     def build(self):\n",
    "#         counter = dict(self.counter.most_common())\n",
    "#         self.word_freq = {\n",
    "#             **{'<PAD>': 0},\n",
    "#             **counter,\n",
    "#         }\n",
    "#         self.token2id = {\n",
    "#             **{'<PAD>': 0},\n",
    "#             **{word: i + 1 for i, word in enumerate(counter)}\n",
    "#         }\n",
    "#         self.lfq = np.array(list(self.word_freq.values())) < self.mincount\n",
    "#         self.hfq = ~self.lfq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_vectors(token2id, path,  limit=None):\n",
    "#     embed_shape = (len(token2id), 100)\n",
    "#     freqs = np.zeros((len(token2id)), dtype='f')\n",
    "\n",
    "#     vectors = np.zeros(embed_shape, dtype='f')\n",
    "#     i = 0\n",
    "#     with open(path, encoding=\"utf8\", errors='ignore') as f:\n",
    "#         for o in f:\n",
    "#             token, *vector = o.split(' ')\n",
    "#             token = str.lower(token)\n",
    "#             if len(o) <= 100:\n",
    "#                 continue\n",
    "#             if limit is not None and i > limit:\n",
    "#                 break\n",
    "#             vectors[token2id[token]] = np.array(vector, 'f')\n",
    "#             i += 1\n",
    "\n",
    "#     return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list_abstracts = df_sub['abstract'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Populating vocab_freq_dict and token2id from my data.\n",
    "# id_ = 0\n",
    "# training_examples = []\n",
    "\n",
    "# # Dictionary to store a unique id for each token in vocab( in my case vocab contains both my vocab and glove vocab)\n",
    "# token2id = {}\n",
    "\n",
    "# # This dictionary will contain all the words and their frequencies.\n",
    "# vocab_freq_dict = {}\n",
    "\n",
    "\n",
    "# for line in tqdm(list_abstracts) :\n",
    "#     words = line.strip().split()\n",
    "#     training_examples.append(words)\n",
    "#     for word in words:\n",
    "#         if word not in vocab_freq_dict:\n",
    "#             vocab_freq_dict.update({word:0})\n",
    "#         vocab_freq_dict[word] += 1\n",
    "#         if word not in token2id:\n",
    "#             token2id.update({word:id_})\n",
    "#             id_ += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding_name = '/Users/s0c02nj/Downloads/glove.6B/glove.6B.100d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import operator\n",
    "\n",
    "# # Populating vocab_freq_dict and token2id from glove vocab.\n",
    "# max_id = max(token2id.items(), key=operator.itemgetter(1))[0]\n",
    "# max_token_id = token2id[max_id]\n",
    "\n",
    "# with open(embedding_name, encoding=\"utf8\", errors='ignore') as f:\n",
    "#     for o in tqdm(f):\n",
    "#         token, *vector = o.split(' ')\n",
    "#         token = str.lower(token)\n",
    "#         if len(o) <= 100:\n",
    "#             continue\n",
    "#         if token not in token2id:\n",
    "#             max_token_id += 1\n",
    "#             token2id.update({token:max_token_id})\n",
    "#             vocab_freq_dict.update({token:1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting vectors to keyedvectors format for gensim\n",
    "# vectors = load_vectors(token2id, embedding_name)\n",
    "# vec = KeyedVectors(100)\n",
    "# vec.add(list(token2id.keys()), vectors, replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting vectors(numpy_array) to None to release memory\n",
    "#vectors = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = dict(min_count=1,workers=6,iter=6,size=100)\n",
    "\n",
    "# model = Word2Vec(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # using build from vocab to build the vocab\n",
    "# model.build_vocab_from_freq(vocab_freq_dict)\n",
    "\n",
    "# # using token2id to create idxmap\n",
    "# idxmap = np.array([token2id[w] for w in model.wv.index2entity])\n",
    "\n",
    "# # Setting hidden weights(syn0 = between input layer and hidden layer) = your vectors arranged accoring to ids\n",
    "# model.wv.vectors[:] = vec.vectors[idxmap]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting hidden weights(syn0 = between hidden layer and output layer) = your vectors arranged accoring to ids\n",
    "#model.trainables.syn1neg[:] = vec.vectors[idxmap]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.train(training_examples, total_examples=len(training_examples), epochs=model.epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.most_similar('virus')[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_path = 'model_dir/final_model.model'\n",
    "# model.save(output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
